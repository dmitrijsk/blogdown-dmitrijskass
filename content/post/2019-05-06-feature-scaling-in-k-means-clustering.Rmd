---
title: 'Scaling of variables in k-means clustering'
author: Dmitrijs Kass
date: '2019-07-27'
slug: feature-scaling-in-k-means-clustering
categories:
  - clusterings
tags: []
output:
  blogdown::html_page:
    toc: true
bibliography: bibliography.bib
---

The process of clustering usually involves variable scaling / standardization. This post illustrates the effect of this data pre-processing step on the result of k-means clustering using a small data set of iPhones.

----

# Clustering and distances

Clustering refers to a very broad set of techniques for finding subgroups, or clusters, in a data set. When we cluster the observations of a data set, we seek to partition them into distinct groups so that 

* the observations within each group are quite "similar" to each other, 
* while observations in different groups are quite "different" from each other [@james_witten_hastie_tibshirani_2017, pp. 385].

There are many possible ways to define the concept of "(dis)similarity", but by far the most common choice involves [Euclidean distance](https://en.wikipedia.org/wiki/Euclidean_distance). If the points $(x_1, y_1)$ and $(x_2,y_2)$ are in 2-dimensional space, then the Euclidean distance between them is

$$\sqrt{(x_2−x_1)^2+(y_2−y_1)^2}\text{.}$$

Let's take k-means clustering. The algorithm aims to partition observations into *k* groups such that the sum of squared *distances* from observations to the assigned cluster centres is minimized. In the example plotted below, if we are looking for two clusters then points A and B would be in one cluster and points C and D in another. 

```{r plot_distance_example, echo = FALSE, message=FALSE, warning=FALSE, fig.height=3, fig.width=4, fig.align='center'}
library(tidyverse)
set.seed(17)
example <- data.frame(x = c(1,2,8,9),
                      y = c(2,2,7,8))
rownames(example) <- LETTERS[1:4]

example %>% 
  rownames_to_column(var = "label") %>% 
  mutate(cluster = c(1,1,2,2)) %>% 
  ggplot(aes(x = x, y = y, label = label, colour = as.factor(cluster))) +
  geom_point(size = 3) +
  ggrepel::geom_text_repel(size = 3.5, show.legend = FALSE)+
  coord_equal() +
  scale_x_continuous(limits = c(0,10), breaks = 1:10, minor_breaks = NULL) +
  scale_y_continuous(limits = c(0,10), breaks = 1:10, minor_breaks = NULL) +
  labs(subtitle = "Closest points form clusters", colour = "cluster")
```


# Surprise

Imagine you have observed prices and weights of four iPhone models. Here is the data (also available [here](iphone_dataset.csv)): 

```{r message=FALSE}
# Attach packages.
library(tidyverse)
# Import phone data.
phones_df <- read_delim(file = "iphone_dataset.csv", delim = ";", skip = 2)
```

```{r echo = FALSE}
# Print phone data.
phones_df %>% 
  knitr::kable(col.names = c("Brand", "Model", "Approx. price (EUR)", "Weight (g)"), format = "html") %>% 
  kableExtra::kable_styling(full_width = FALSE, bootstrap_options = c("striped", "condensed"))
```

The plot below depicts four iPhone models in two dimensions: approximate price in euro and weight in grams. Please do a mental exercise and assign these four iPhone models into two clusters. Which phones did you assign into cluster 1 and which into cluster 2?

```{r plot_surprise_1_question, echo = FALSE, fig.height=3.5, fig.width=7, fig.align='center'}
phones_df %>% 
  ggplot(aes(x = weight_g, y = approx_price_eur)) + 
  geom_point(size = 4) +
  geom_text(aes(label = model), size = 3.5, vjust = "top", nudge_y = -40) +
  scale_x_continuous(expand = expand_scale(mult = 0.3)) +
  scale_y_continuous(expand = expand_scale(mult = 0.3)) +
  labs(subtitle = "How would you assign these four iPhones models into two clusters?",
       x = "weight (g)", 
       y = "price (EUR)") +
  theme_classic()
```

Did you assign iPhone 6 and 4 into one cluster and iPhone 8 Plus and 6s Plus into another? That's not what k-mean would do it without variable scaling.



# K-means with original variables

Below is the result from k-means. iPhone 8 Plus is alone in one cluster and three other phones are in another cluster:

```{r}
# Data for clusering.
phones_cl <- phones_df[c("approx_price_eur", "weight_g")]
# Reproducibility.
set.seed(1)
# K-means with original variables.
kmeans_fit <- kmeans(phones_cl, centers = 2)
```

```{r plot_kmeans_orig, echo = FALSE, fig.height=3.5, fig.width=7, fig.align='center'}
# Small data frame with cluster centers.
centers_df <- as_tibble(kmeans_fit$centers) %>%
  mutate(cluster = 1:2)

# K-means with original variables, unequal axes.
p <- phones_df %>% 
  mutate(cluster = kmeans_fit$cluster) %>% 
  ggplot(aes(x = weight_g, y = approx_price_eur, color = as.factor(cluster))) + 
  geom_point(size = 3, show.legend = FALSE) +
  geom_point(data = centers_df, size = 6, shape = "x") +
  scale_x_continuous(expand = expand_scale(mult = 0.3)) +
  scale_y_continuous(expand = expand_scale(mult = 0.3)) +
  labs(title = "Hopefully, you are surprised to see iPhone 8 Plus alone in cluster 2",
       subtitle = "k-means clustering with original variables",
       x = "weight (g)", 
       y = "price (EUR)", 
       color = "Cluster centers") +
  theme_classic() +
  theme(legend.position = "top")

p +
  geom_text(aes(label = model), size = 3.5, hjust = "left", nudge_x = 2, show.legend = FALSE)
```


```{r echo = FALSE}
price_range_vec <- range(phones_df$approx_price_eur)
min_price <- price_range_vec[1]
max_price <- price_range_vec[2]
price_range <- diff(price_range_vec)

weight_range_vec <- range(phones_df$weight_g)
min_weight <- weight_range_vec[1]
max_weight <- weight_range_vec[2]
weight_range <- diff(weight_range_vec)

range_ratio <- round(price_range / weight_range, 1)
```

The trick is in the variability of prices and weights. The range of prices is `r min_price` - `r max_price` = `r price_range`, while the range of weights is `r min_weight` - `r max_weight` = `r weight_range`. The price range is `r range_ratio` times greater than the weight range. It means that in a calculation of Euclidean distances, 1 euro difference in price is equivalent to `r range_ratio` difference in weight. Thus, the price affects the clustering result more than the weight.

If we re-plot the same data set while ensuring that one unit on the x-axis (euros) is the same length as one unit on the y-axis (grams), the k-means result is no more a surprise:

```{r plot_kmeans_orig_coord_equal, echo = FALSE, fig.height=3.5, fig.width=7, fig.align='center', message=FALSE}
# Adjust axes to the same scale.
p_1 <- p +
  geom_text(aes(label = model), size = 3.5, hjust = "left", nudge_x = 60, show.legend = FALSE) +
  labs(title = "No more surprise after equally scaling axes") +
  coord_equal(xlim = c(0, 800), ylim = c(0, 800), expand = FALSE)

p_1
```

Here are pairwise Euclidean distances calculated with `dist()`:

```{r}
# Add rownames to see phone models in a distance matrix.
phones_cl <- as.data.frame(phones_cl)
rownames(phones_cl) <- phones_df$model
# Euclidean distances.
round(dist(phones_cl), 1)
```

Notice how far iPhone 8 Plus is from all other phones.


# K-means with standardized variables

Now consider standardizing each variable by subtracting the mean $\bar{x}$ (i.e. centering) and dividing by its standard deviation $s$:

$$Z=\frac{X-\bar{x}}{s}.$$

```{r}
phones_cl_scaled <- scale(phones_cl)
round(dist(phones_cl_scaled), 1)
```

The ranks of distances have changed and so do k-means results:

```{r}
kmeans_fit <- kmeans(phones_cl_scaled, centers = 2)
```

```{r plot_kmeans_scaled_standardized_measurements, echo = FALSE, fig.height=3.5, fig.width=7, fig.align='center', message=FALSE}
feature_centers_vec <- attributes(phones_cl_scaled)[["scaled:center"]]
feature_sd_vec      <- attributes(phones_cl_scaled)[["scaled:scale"]]

centers_df <- tibble(approx_price_eur_scaled = kmeans_fit$centers[, 1],
       weight_g_scaled = kmeans_fit$centers[, 2]) %>%
  bind_cols(as_tibble(t(apply(X = kmeans_fit$centers, MARGIN = 1, FUN = function(x) x * feature_sd_vec + feature_centers_vec)))) %>%
  mutate(cluster = 1:2)

phones_df %>%
  mutate(approx_price_eur_scaled = (approx_price_eur - mean(approx_price_eur)) / sd(approx_price_eur),
         weight_g_scaled = (weight_g - mean(weight_g)) / sd(weight_g),
         cluster = kmeans_fit$cluster) %>%
  ggplot(aes(x = weight_g_scaled, y = approx_price_eur_scaled, color = as.factor(cluster))) +
  geom_point(size = 3, show.legend = FALSE) +
  geom_point(data = centers_df, aes(x = weight_g_scaled, y = approx_price_eur_scaled), size = 6, shape = "x") +
  labs(title = "Same scale axes, standardized units",
       subtitle = "k-means clustering with scaled variables",
       x = "standardized weight",
       y = "standardized price", color = "Cluster centers") +
  theme_classic() +
  theme(legend.position = "top") +
  coord_equal(xlim = c(-2, 2), ylim = c(-2, 2), clip = "off") +
  geom_text(aes(label = model), size = 3.5, hjust = "left", nudge_x = 0.3, show.legend = FALSE)
```

And the same with variables scaled back to original measurements:

```{r plot_kmeans_scaled_orig_measurements, echo = FALSE, fig.height=3.5, fig.width=7, fig.align='center', message=FALSE}
p_2 <- phones_df %>%
  mutate(cluster = kmeans_fit$cluster) %>%
  ggplot(aes(x = weight_g, y = approx_price_eur, color = as.factor(cluster))) +
  geom_point(size = 3, show.legend = FALSE) +
  geom_text(aes(label = model), size = 3.5, hjust = "left", nudge_x = 60, show.legend = FALSE) +
  geom_point(data = centers_df, aes(x = weight_g, y = approx_price_eur), size = 6, shape = "x") +
  labs(title = "Same scale axes, original units",
       subtitle = "k-means clustering with scaled variable",
       x = "weight (g)",
       y = "price (EUR)",
       color = "Cluster centers") +
  theme_classic() +
  theme(legend.position = "top") +
  coord_equal(xlim = c(0, 800), ylim = c(0, 800))

p_2
```

```{r plot_combined, echo = FALSE, fig.height=3.5, fig.width=7, fig.align='center', message=FALSE}
p3 <- gridExtra::grid.arrange(p_1 + 
                          labs(title = expression(paste("k-means, ", k==2)), 
                                   subtitle = "Using original variables") +
                          theme(plot.subtitle = element_text(face = "bold")), 
                        p_2 + 
                          labs(title = "", 
                                   subtitle = "Using standardized variables") +
                          theme(plot.subtitle = element_text(face = "bold")), 
                        nrow = 1, ncol = 2)
ggsave(filename = "p3.png", plot = p3)
```

<hr>

I would appreciate any comments or suggestions. Please leave them below, no login required if you check "I'd rather post as a guest".

# References
