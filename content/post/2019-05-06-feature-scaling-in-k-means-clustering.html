---
title: 'Scaling of variables in k-means clustering'
author: Dmitrijs Kass
date: '2019-07-27'
slug: feature-scaling-in-k-means-clustering
categories:
  - clusterings
tags: []
output:
  blogdown::html_page:
    toc: true
bibliography: bibliography.bib
---

<script src="/rmarkdown-libs/kePrint/kePrint.js"></script>

<div id="TOC">
<ul>
<li><a href="#clustering-and-distances">Clustering and distances</a></li>
<li><a href="#surprise">Surprise</a></li>
<li><a href="#k-means-with-original-variables">K-means with original variables</a></li>
<li><a href="#k-means-with-standardized-variables">K-means with standardized variables</a></li>
<li><a href="#references">References</a></li>
</ul>
</div>

<p>The process of clustering usually involves variable scaling / standardization. This post illustrates the effect of this data pre-processing step on the result of k-means clustering using a small data set of iPhones.</p>
<hr />
<div id="clustering-and-distances" class="section level1">
<h1>Clustering and distances</h1>
<p>Clustering refers to a very broad set of techniques for finding subgroups, or clusters, in a data set. When we cluster the observations of a data set, we seek to partition them into distinct groups so that</p>
<ul>
<li>the observations within each group are quite “similar” to each other,</li>
<li>while observations in different groups are quite “different” from each other <span class="citation">(James et al. 2017, 385)</span>.</li>
</ul>
<p>There are many possible ways to define the concept of “(dis)similarity”, but by far the most common choice involves <a href="https://en.wikipedia.org/wiki/Euclidean_distance">Euclidean distance</a>. If the points <span class="math inline">\((x_1, y_1)\)</span> and <span class="math inline">\((x_2,y_2)\)</span> are in 2-dimensional space, then the Euclidean distance between them is</p>
<p><span class="math display">\[\sqrt{(x_2−x_1)^2+(y_2−y_1)^2}\text{.}\]</span></p>
<p>Let’s take k-means clustering. The algorithm aims to partition observations into <em>k</em> groups such that the sum of squared <em>distances</em> from observations to the assigned cluster centres is minimized. In the example plotted below, if we are looking for two clusters then points A and B would be in one cluster and points C and D in another.</p>
<p><img src="/post/2019-05-06-feature-scaling-in-k-means-clustering_files/figure-html/plot_distance_example-1.png" width="384" style="display: block; margin: auto;" /></p>
</div>
<div id="surprise" class="section level1">
<h1>Surprise</h1>
<p>Imagine you have observed prices and weights of four iPhone models. Here is the data (also available <a href="iphone_dataset.csv">here</a>):</p>
<pre class="r"><code># Attach packages.
library(tidyverse)
# Import phone data.
phones_df &lt;- read_delim(file = &quot;iphone_dataset.csv&quot;, delim = &quot;;&quot;, skip = 2)</code></pre>
<table class="table table-striped table-condensed" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
Brand
</th>
<th style="text-align:left;">
Model
</th>
<th style="text-align:right;">
Approx. price (EUR)
</th>
<th style="text-align:right;">
Weight (g)
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Apple
</td>
<td style="text-align:left;">
iPhone 4
</td>
<td style="text-align:right;">
200
</td>
<td style="text-align:right;">
137
</td>
</tr>
<tr>
<td style="text-align:left;">
Apple
</td>
<td style="text-align:left;">
iPhone 6
</td>
<td style="text-align:right;">
360
</td>
<td style="text-align:right;">
129
</td>
</tr>
<tr>
<td style="text-align:left;">
Apple
</td>
<td style="text-align:left;">
iPhone 6s Plus
</td>
<td style="text-align:right;">
470
</td>
<td style="text-align:right;">
192
</td>
</tr>
<tr>
<td style="text-align:left;">
Apple
</td>
<td style="text-align:left;">
iPhone 8 Plus
</td>
<td style="text-align:right;">
770
</td>
<td style="text-align:right;">
202
</td>
</tr>
</tbody>
</table>
<p>The plot below depicts four iPhone models in two dimensions: approximate price in euro and weight in grams. Please do a mental exercise and assign these four iPhone models into two clusters. Which phones did you assign into cluster 1 and which into cluster 2?</p>
<p><img src="/post/2019-05-06-feature-scaling-in-k-means-clustering_files/figure-html/plot_surprise_1_question-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Did you assign iPhone 6 and 4 into one cluster and iPhone 8 Plus and 6s Plus into another? That’s not what k-mean would do it without variable scaling.</p>
</div>
<div id="k-means-with-original-variables" class="section level1">
<h1>K-means with original variables</h1>
<p>Below is the result from k-means. iPhone 8 Plus is alone in one cluster and three other phones are in another cluster:</p>
<pre class="r"><code># Data for clusering.
phones_cl &lt;- phones_df[c(&quot;approx_price_eur&quot;, &quot;weight_g&quot;)]
# Reproducibility.
set.seed(1)
# K-means with original variables.
kmeans_fit &lt;- kmeans(phones_cl, centers = 2)</code></pre>
<p><img src="/post/2019-05-06-feature-scaling-in-k-means-clustering_files/figure-html/plot_kmeans_orig-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>The trick is in the variability of prices and weights. The range of prices is 200 - 770 = 570, while the range of weights is 129 - 202 = 73. The price range is 7.8 times greater than the weight range. It means that in a calculation of Euclidean distances, 1 euro difference in price is equivalent to 7.8 difference in weight. Thus, the price affects the clustering result more than the weight.</p>
<p>If we re-plot the same data set while ensuring that one unit on the x-axis (euros) is the same length as one unit on the y-axis (grams), the k-means result is no more a surprise:</p>
<p><img src="/post/2019-05-06-feature-scaling-in-k-means-clustering_files/figure-html/plot_kmeans_orig_coord_equal-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Here are pairwise Euclidean distances calculated with <code>dist()</code>:</p>
<pre class="r"><code># Add rownames to see phone models in a distance matrix.
phones_cl &lt;- as.data.frame(phones_cl)
rownames(phones_cl) &lt;- phones_df$model
# Euclidean distances.
round(dist(phones_cl), 1)</code></pre>
<pre><code>##                iPhone 4 iPhone 6 iPhone 6s Plus
## iPhone 6          160.2                        
## iPhone 6s Plus    275.5    126.8               
## iPhone 8 Plus     573.7    416.4          300.2</code></pre>
<p>Notice how far iPhone 8 Plus is from all other phones.</p>
</div>
<div id="k-means-with-standardized-variables" class="section level1">
<h1>K-means with standardized variables</h1>
<p>Now consider standardizing each variable by subtracting the mean <span class="math inline">\(\bar{x}\)</span> (i.e. centering) and dividing by its standard deviation <span class="math inline">\(s\)</span>:</p>
<p><span class="math display">\[Z=\frac{X-\bar{x}}{s}.\]</span></p>
<pre class="r"><code>phones_cl_scaled &lt;- scale(phones_cl)
round(dist(phones_cl_scaled), 1)</code></pre>
<pre><code>##                iPhone 4 iPhone 6 iPhone 6s Plus
## iPhone 6            0.7                        
## iPhone 6s Plus      1.9      1.7               
## iPhone 8 Plus       2.9      2.6            1.3</code></pre>
<p>The ranks of distances have changed and so do k-means results:</p>
<pre class="r"><code>kmeans_fit &lt;- kmeans(phones_cl_scaled, centers = 2)</code></pre>
<p><img src="/post/2019-05-06-feature-scaling-in-k-means-clustering_files/figure-html/plot_kmeans_scaled_standardized_measurements-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>And the same with variables scaled back to original measurements:</p>
<p><img src="/post/2019-05-06-feature-scaling-in-k-means-clustering_files/figure-html/plot_kmeans_scaled_orig_measurements-1.png" width="672" style="display: block; margin: auto;" /></p>
<p><img src="/post/2019-05-06-feature-scaling-in-k-means-clustering_files/figure-html/plot_combined-1.png" width="672" style="display: block; margin: auto;" /></p>
<hr>
<p>I would appreciate any comments or suggestions. Please leave them below, no login required if you check “I’d rather post as a guest”.</p>
</div>
<div id="references" class="section level1 unnumbered">
<h1>References</h1>
<div id="refs" class="references">
<div id="ref-james_witten_hastie_tibshirani_2017">
<p>James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2017. <em>An Introduction to Statistical Learning: With Applications in R</em>. Springer. <a href="http://faculty.marshall.usc.edu/gareth-james/ISL/">http://faculty.marshall.usc.edu/gareth-james/ISL/</a>.</p>
</div>
</div>
</div>
