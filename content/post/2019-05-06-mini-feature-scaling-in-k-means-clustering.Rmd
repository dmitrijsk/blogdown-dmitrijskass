---
title: 'Mini: feature scaling in k-means clustering'
author: Dmitrijs Kass
date: '2019-05-06'
slug: mini-feature-scaling-in-k-means-clustering
categories:
  - mini
  - clusterings
tags: []
output:
  blogdown::html_page:
    toc: true
draft: true
---

The process of clustering usually involves variable scaling / standardization. This post illustrates the effect and importance of this date pre-processing step on a small data set of iPhones.

----

# Clustering and distances

Clustering refers to a very broad set of techniques for finding subgroups, or clusters, in a data set. When we cluster the observations of a data set, we seek to partition them into distinct groups so that 

* the observations within each group are quite "similar" to each other, 
* while observations in different groups are quite "different" from each other (... page 385).

There are many possible ways to define the concept of "(dis)similarity", but by far the most common choice involves [Euclidean distance](https://en.wikipedia.org/wiki/Euclidean_distance). For example, if three points have (x, y) coordinates (1, 1), (1, 2) and (3, 1) then their Euclidean distances are as shown in the picture below:

```{r echo = FALSE, message=FALSE, warning=FALSE, fig.height=3, fig.width=3, fig.align='center'}
library(tidyverse)
example <- tibble(object = LETTERS[1:3],
                  x = c(1,1,3),
                  xend = c(1,3,1),
                  y = c(1,2,1),
                  yend = c(2,1,1))

distances <- dist(example[c("x", "y")])

example %>% 
  ggplot(aes(x, y)) +
  geom_point(size = 2) +
  coord_equal() +
  geom_segment(aes(xend = xend, yend = yend), 
               arrow = arrow(length = unit(0.3, "cm"))) +
  scale_x_continuous(breaks = 1:3, expand = expand_scale(add = 0.25)) +
  scale_y_continuous(breaks = 1:3, expand = expand_scale(add = 0.25)) +
  annotate(geom = "text", x = 0.8, y = 1.5, label = 1) +
  annotate(geom = "text", x = 2.2, y = 1.7, label = 2.24) +
  annotate(geom = "text", x = 2, y = 0.8, label = 2) +
  labs(subtitle = "Euclidean distances")
```

<!-- Standardization is not important if the distance measure is already standardized, e.g. correlation and cosine distance.  -->

Let's take k-means clustering. Simply speaking, k-means algorithm aims to assign *n* observations into *k* clusters such that within-cluster distances are minimized. It is usually advised to standardize variables before calculating the distances. Now, imagine you have observed their prices in euros and weights in gramms. Here is the data (also available [here](iphone_dataset.csv)): 

```{r message=FALSE, warning=FALSE}
# Attach packages.
library(tidyverse)
# Import phone data.
phones_df <- read_delim(file = "iphone_dataset.csv", delim = ";", skip = 2)
```

```{r echo = FALSE}
# Print phone data.
phones_df %>% 
  knitr::kable(col.names = c("Brand", "Model", "Approx. price (EUR)", "Weight (g)"), format = "html") %>% 
  kableExtra::kable_styling(full_width = FALSE, bootstrap_options = c("striped", "condensed"))
```

# About k-means


# Surprise

The plot below depicts four iPhone models in two dimensions: approximate price in euro and weight in grams. How would you assign these four iPhones into two clusters?

```{r fig.height=3, fig.width=6}
phones_df %>% 
  ggplot(aes(x = weight_g, y = approx_price_eur)) + 
  geom_point(size = 4, shape = "x") +
  geom_text(aes(label = model), size = 3.5, vjust = "top", nudge_y = -40) +
  scale_x_continuous(expand = expand_scale(mult = 0.3)) +
  scale_y_continuous(expand = expand_scale(mult = 0.3)) +
  labs(title = "How would you assign these 4 iPhones into 2 clusters?",
       x = "weight (g)", 
       y = "price (EUR)") +
  theme_classic()
```

# K-means clustering with original features

```{r}
# Save columns for clustering in a separate matrix.
phones_mat <- as.matrix(phones_df %>% select(approx_price_eur, weight_g))

# Add rownames to see phone models in a distance matrix.
rownames(phones_mat) <- phones_df$model
```

```{r}
# k-means.
set.seed(1)
kmeans_fit <- kmeans(phones_mat, centers = 2, nstart = 100)

# Small data frame with cluster centers.
centers_df <- as_tibble(kmeans_fit$centers) %>%
  mutate(cluster = 1:2)
```

```{r fig.height=3.5, fig.width=6}
p <- phones_df %>% 
  mutate(cluster = kmeans_fit$cluster) %>% 
  ggplot(aes(x = weight_g, y = approx_price_eur, color = as.factor(cluster))) + 
  geom_point(size = 4, shape = "x") +
  geom_text(aes(label = model), size = 3.5, vjust = "top", nudge_y = -40) +
  geom_point(data = centers_df, size = 4) +
  scale_x_continuous(expand = expand_scale(mult = 0.3)) +
  scale_y_continuous(expand = expand_scale(mult = 0.3)) +
  labs(title = "Hopefully, you are surprised to see iPhone 8 Plus alone in cluster 2",
       subtitle = "k-means clustering with original (not scaled) features",
       x = "weight (g)", 
       y = "price (EUR)", 
       color = "Cluster centers") +
  theme_classic() +
  theme(legend.position = "top")

p
```

```{r fig.height=4, fig.width=6, message=FALSE}
# Adjust axes to the same scale.
p +
  labs(title = "No more surprise after equally scaling axes") +
  coord_equal(xlim = c(0, 800), ylim = c(0, 800), expand = FALSE)
```

# Feature scaling and k-means revisited

Below are Eucledean distances between phones using original features. Values of distances are not interpretable per se because those are euros and grams combined. Magnitudes is what we are interested in. Notice that the distance between "Galaxy S8+" and "Galaxy A7" (190.44) is the second largest distance.

```{r}
round(dist(phones_mat), 2)
```

Now compare it with the same distance calculated from scaled features. It is now 

Distance matrix after feature scaling.

```{r}
phones_mat_scaled <- scale(phones_mat)
round(dist(phones_mat_scaled), 2)
```

```{r}
set.seed(2)
kmeans_fit <- kmeans(phones_mat_scaled, centers = 2, nstart = 100)

feature_centers_vec <- attributes(phones_mat_scaled)[["scaled:center"]]
feature_sd_vec      <- attributes(phones_mat_scaled)[["scaled:scale"]]

centers_df <- tibble(approx_price_eur_scaled = kmeans_fit$centers[, 1],
       weight_g_scaled = kmeans_fit$centers[, 2]) %>% 
  bind_cols(as_tibble(t(apply(X = kmeans_fit$centers, MARGIN = 1, FUN = function(x) x * feature_sd_vec + feature_centers_vec)))) %>% 
  mutate(cluster = 1:2)
```

```{r}
p <- phones_df %>% 
  mutate(approx_price_eur_scaled = (approx_price_eur - mean(approx_price_eur)) / sd(approx_price_eur), 
         weight_g_scaled = (weight_g - mean(weight_g)) / sd(weight_g),
         cluster = kmeans_fit$cluster) %>% 
  ggplot(aes(x = weight_g_scaled, y = approx_price_eur_scaled, color = as.factor(cluster))) + 
  geom_point(size = 4, shape = "x") +
  geom_text(aes(label = model), size = 3.5, vjust = "top", nudge_y = -0.1) +
  
  geom_point(data = centers_df, aes(x = weight_g_scaled, y = approx_price_eur_scaled), size = 4) +
  labs(title = "Now differences in price and weight are standardized",
       subtitle = "k-means clustering with scaled features",
       x = "standardized weight", 
       y = "standardized price", color = "Cluster centers") +
  theme_classic() +
  theme(legend.position = "top") +
  coord_equal(xlim = c(-2, 2), ylim = c(-2, 2))


p +
  stat_ellipse(type = "norm")

```

After getting k-means result based on scaled features, we may scaled them back.

```{r}
phones_df %>% 
  mutate(cluster = kmeans_fit$cluster) %>% 
  ggplot(aes(x = weight_g, y = approx_price_eur, color = as.factor(cluster))) + 
  geom_point(size = 4, shape = "x") +
  geom_text(aes(label = model), size = 3.5, vjust = "top", nudge_y = -20) +
  geom_point(data = centers_df, aes(x = weight_g, y = approx_price_eur), size = 8, shape = "x") +
  labs(title = "",
       subtitle = "k-means clustering with scaled features",
       x = "weight (g)",
       y = "price (EUR)", 
       color = "Cluster centers") +
  theme_classic() +
  theme(legend.position = "top") +
  coord_equal(xlim = c(0, 800), ylim = c(0, 800))
```


