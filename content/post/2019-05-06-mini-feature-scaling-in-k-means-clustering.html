---
title: 'Mini: feature scaling in k-means clustering'
author: Dmitrijs Kass
date: '2019-05-06'
slug: mini-feature-scaling-in-k-means-clustering
categories:
  - mini
  - clusterings
tags: []
output:
  blogdown::html_page:
    toc: true
draft: true
---

<script src="/rmarkdown-libs/kePrint/kePrint.js"></script>

<div id="TOC">
<ul>
<li><a href="#clustering-and-distances">Clustering and distances</a></li>
<li><a href="#about-k-means">About k-means</a></li>
<li><a href="#surprise">Surprise</a></li>
<li><a href="#k-means-clustering-with-original-features">K-means clustering with original features</a></li>
<li><a href="#feature-scaling-and-k-means-revisited">Feature scaling and k-means revisited</a></li>
</ul>
</div>

<p>The process of clustering usually involves variable scaling / standardization. This post illustrates the effect and importance of this date pre-processing step on a small data set of iPhones.</p>
<hr />
<div id="clustering-and-distances" class="section level1">
<h1>Clustering and distances</h1>
<p>Clustering refers to a very broad set of techniques for finding subgroups, or clusters, in a data set. When we cluster the observations of a data set, we seek to partition them into distinct groups so that</p>
<ul>
<li>the observations within each group are quite “similar” to each other,</li>
<li>while observations in different groups are quite “different” from each other (… page 385).</li>
</ul>
<p>There are many possible ways to define the concept of “(dis)similarity”, but by far the most common choice involves <a href="https://en.wikipedia.org/wiki/Euclidean_distance">Euclidean distance</a>. For example, if three points have (x, y) coordinates (1, 1), (1, 2) and (3, 1) then their Euclidean distances are as shown in the picture below:</p>
<p><img src="/post/2019-05-06-mini-feature-scaling-in-k-means-clustering_files/figure-html/unnamed-chunk-1-1.png" width="288" style="display: block; margin: auto;" /></p>
<!-- Standardization is not important if the distance measure is already standardized, e.g. correlation and cosine distance.  -->
<p>Let’s take k-means clustering. Simply speaking, k-means algorithm aims to assign <em>n</em> observations into <em>k</em> clusters such that within-cluster distances are minimized. It is usually advised to standardize variables before calculating the distances. Now, imagine you have observed their prices in euros and weights in gramms. Here is the data (also available <a href="iphone_dataset.csv">here</a>):</p>
<pre class="r"><code># Attach packages.
library(tidyverse)
# Import phone data.
phones_df &lt;- read_delim(file = &quot;iphone_dataset.csv&quot;, delim = &quot;;&quot;, skip = 2)</code></pre>
<table class="table table-striped table-condensed" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
Brand
</th>
<th style="text-align:left;">
Model
</th>
<th style="text-align:right;">
Approx. price (EUR)
</th>
<th style="text-align:right;">
Weight (g)
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Apple
</td>
<td style="text-align:left;">
iPhone 4
</td>
<td style="text-align:right;">
200
</td>
<td style="text-align:right;">
137
</td>
</tr>
<tr>
<td style="text-align:left;">
Apple
</td>
<td style="text-align:left;">
iPhone 6
</td>
<td style="text-align:right;">
360
</td>
<td style="text-align:right;">
129
</td>
</tr>
<tr>
<td style="text-align:left;">
Apple
</td>
<td style="text-align:left;">
iPhone 6s Plus
</td>
<td style="text-align:right;">
470
</td>
<td style="text-align:right;">
192
</td>
</tr>
<tr>
<td style="text-align:left;">
Apple
</td>
<td style="text-align:left;">
iPhone 8 Plus
</td>
<td style="text-align:right;">
770
</td>
<td style="text-align:right;">
202
</td>
</tr>
</tbody>
</table>
</div>
<div id="about-k-means" class="section level1">
<h1>About k-means</h1>
</div>
<div id="surprise" class="section level1">
<h1>Surprise</h1>
<p>The plot below depicts four iPhone models in two dimensions: approximate price in euro and weight in grams. How would you assign these four iPhones into two clusters?</p>
<pre class="r"><code>phones_df %&gt;% 
  ggplot(aes(x = weight_g, y = approx_price_eur)) + 
  geom_point(size = 4, shape = &quot;x&quot;) +
  geom_text(aes(label = model), size = 3.5, vjust = &quot;top&quot;, nudge_y = -40) +
  scale_x_continuous(expand = expand_scale(mult = 0.3)) +
  scale_y_continuous(expand = expand_scale(mult = 0.3)) +
  labs(title = &quot;How would you assign these 4 iPhones into 2 clusters?&quot;,
       x = &quot;weight (g)&quot;, 
       y = &quot;price (EUR)&quot;) +
  theme_classic()</code></pre>
<p><img src="/post/2019-05-06-mini-feature-scaling-in-k-means-clustering_files/figure-html/unnamed-chunk-4-1.png" width="576" /></p>
</div>
<div id="k-means-clustering-with-original-features" class="section level1">
<h1>K-means clustering with original features</h1>
<pre class="r"><code># Save columns for clustering in a separate matrix.
phones_mat &lt;- as.matrix(phones_df %&gt;% select(approx_price_eur, weight_g))

# Add rownames to see phone models in a distance matrix.
rownames(phones_mat) &lt;- phones_df$model</code></pre>
<pre class="r"><code># k-means.
set.seed(1)
kmeans_fit &lt;- kmeans(phones_mat, centers = 2, nstart = 100)

# Small data frame with cluster centers.
centers_df &lt;- as_tibble(kmeans_fit$centers) %&gt;%
  mutate(cluster = 1:2)</code></pre>
<pre class="r"><code>p &lt;- phones_df %&gt;% 
  mutate(cluster = kmeans_fit$cluster) %&gt;% 
  ggplot(aes(x = weight_g, y = approx_price_eur, color = as.factor(cluster))) + 
  geom_point(size = 4, shape = &quot;x&quot;) +
  geom_text(aes(label = model), size = 3.5, vjust = &quot;top&quot;, nudge_y = -40) +
  geom_point(data = centers_df, size = 4) +
  scale_x_continuous(expand = expand_scale(mult = 0.3)) +
  scale_y_continuous(expand = expand_scale(mult = 0.3)) +
  labs(title = &quot;Hopefully, you are surprised to see iPhone 8 Plus alone in cluster 2&quot;,
       subtitle = &quot;k-means clustering with original (not scaled) features&quot;,
       x = &quot;weight (g)&quot;, 
       y = &quot;price (EUR)&quot;, 
       color = &quot;Cluster centers&quot;) +
  theme_classic() +
  theme(legend.position = &quot;top&quot;)

p</code></pre>
<p><img src="/post/2019-05-06-mini-feature-scaling-in-k-means-clustering_files/figure-html/unnamed-chunk-7-1.png" width="576" /></p>
<pre class="r"><code># Adjust axes to the same scale.
p +
  labs(title = &quot;No more surprise after equally scaling axes&quot;) +
  coord_equal(xlim = c(0, 800), ylim = c(0, 800), expand = FALSE)</code></pre>
<p><img src="/post/2019-05-06-mini-feature-scaling-in-k-means-clustering_files/figure-html/unnamed-chunk-8-1.png" width="576" /></p>
</div>
<div id="feature-scaling-and-k-means-revisited" class="section level1">
<h1>Feature scaling and k-means revisited</h1>
<p>Below are Eucledean distances between phones using original features. Values of distances are not interpretable per se because those are euros and grams combined. Magnitudes is what we are interested in. Notice that the distance between “Galaxy S8+” and “Galaxy A7” (190.44) is the second largest distance.</p>
<pre class="r"><code>round(dist(phones_mat), 2)</code></pre>
<pre><code>##                iPhone 4 iPhone 6 iPhone 6s Plus
## iPhone 6         160.20                        
## iPhone 6s Plus   275.54   126.76               
## iPhone 8 Plus    573.69   416.45         300.17</code></pre>
<p>Now compare it with the same distance calculated from scaled features. It is now</p>
<p>Distance matrix after feature scaling.</p>
<pre class="r"><code>phones_mat_scaled &lt;- scale(phones_mat)
round(dist(phones_mat_scaled), 2)</code></pre>
<pre><code>##                iPhone 4 iPhone 6 iPhone 6s Plus
## iPhone 6           0.70                        
## iPhone 6s Plus     1.85     1.75               
## iPhone 8 Plus      2.94     2.60           1.28</code></pre>
<pre class="r"><code>set.seed(2)
kmeans_fit &lt;- kmeans(phones_mat_scaled, centers = 2, nstart = 100)

feature_centers_vec &lt;- attributes(phones_mat_scaled)[[&quot;scaled:center&quot;]]
feature_sd_vec      &lt;- attributes(phones_mat_scaled)[[&quot;scaled:scale&quot;]]

centers_df &lt;- tibble(approx_price_eur_scaled = kmeans_fit$centers[, 1],
       weight_g_scaled = kmeans_fit$centers[, 2]) %&gt;% 
  bind_cols(as_tibble(t(apply(X = kmeans_fit$centers, MARGIN = 1, FUN = function(x) x * feature_sd_vec + feature_centers_vec)))) %&gt;% 
  mutate(cluster = 1:2)</code></pre>
<pre class="r"><code>p &lt;- phones_df %&gt;% 
  mutate(approx_price_eur_scaled = (approx_price_eur - mean(approx_price_eur)) / sd(approx_price_eur), 
         weight_g_scaled = (weight_g - mean(weight_g)) / sd(weight_g),
         cluster = kmeans_fit$cluster) %&gt;% 
  ggplot(aes(x = weight_g_scaled, y = approx_price_eur_scaled, color = as.factor(cluster))) + 
  geom_point(size = 4, shape = &quot;x&quot;) +
  geom_text(aes(label = model), size = 3.5, vjust = &quot;top&quot;, nudge_y = -0.1) +
  
  geom_point(data = centers_df, aes(x = weight_g_scaled, y = approx_price_eur_scaled), size = 4) +
  labs(title = &quot;Now differences in price and weight are standardized&quot;,
       subtitle = &quot;k-means clustering with scaled features&quot;,
       x = &quot;standardized weight&quot;, 
       y = &quot;standardized price&quot;, color = &quot;Cluster centers&quot;) +
  theme_classic() +
  theme(legend.position = &quot;top&quot;) +
  coord_equal(xlim = c(-2, 2), ylim = c(-2, 2))


p +
  stat_ellipse(type = &quot;norm&quot;)</code></pre>
<pre><code>## Too few points to calculate an ellipse
## Too few points to calculate an ellipse</code></pre>
<pre><code>## Warning: Removed 2 rows containing missing values (geom_path).</code></pre>
<p><img src="/post/2019-05-06-mini-feature-scaling-in-k-means-clustering_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p>After getting k-means result based on scaled features, we may scaled them back.</p>
<pre class="r"><code>phones_df %&gt;% 
  mutate(cluster = kmeans_fit$cluster) %&gt;% 
  ggplot(aes(x = weight_g, y = approx_price_eur, color = as.factor(cluster))) + 
  geom_point(size = 4, shape = &quot;x&quot;) +
  geom_text(aes(label = model), size = 3.5, vjust = &quot;top&quot;, nudge_y = -20) +
  geom_point(data = centers_df, aes(x = weight_g, y = approx_price_eur), size = 8, shape = &quot;x&quot;) +
  labs(title = &quot;&quot;,
       subtitle = &quot;k-means clustering with scaled features&quot;,
       x = &quot;weight (g)&quot;,
       y = &quot;price (EUR)&quot;, 
       color = &quot;Cluster centers&quot;) +
  theme_classic() +
  theme(legend.position = &quot;top&quot;) +
  coord_equal(xlim = c(0, 800), ylim = c(0, 800))</code></pre>
<p><img src="/post/2019-05-06-mini-feature-scaling-in-k-means-clustering_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
</div>
