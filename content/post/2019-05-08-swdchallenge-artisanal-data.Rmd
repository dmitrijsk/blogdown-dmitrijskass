---
title: '#SWDchallenge: artisanal data'
author: Dmitrijs Kass
date: '2019-05-10'
categories:
  - data-viz
  - text analysis
tags:
  - SWDchallenge
slug: swdchallenge-artisanal-data
output:
  blogdown::html_page:
    toc: yes
---

This post is a participation in [#SWDchallenge: artisanal data](http://www.storytellingwithdata.com/blog/2019/5/1/swdchallenge-artisanal-data). Along with that it **visualizes two common measures of word importance: tf and tf-idf**. Variations of tf-idf, term frequency-inverse document frequency, are often used by search engines and text-based recommender systems.

----

I use the [transcript](https://docs.google.com/document/d/1S2_63MUbvQs7fxWQrcuCNSl03fmDl1Vr3FjNjnbWK14/edit#) of the "learning dataviz" episode of [#SWDpodcast](http://www.storytellingwithdata.com/podcast), where 12 data visualization professionals share their stories and recommendations.

I find a lot of wisdom and inspiration in this episode. [Storytelling with Data](http://www.storytellingwithdata.com/book) (SWD) book has been on my shelf for over a year and it is this podcast episode that sparkled interest in me about data visualization and made me read the book in few days and start participating in data viz challenges. So I thought I would enjoy spending more time with this episode by analysing the words in it.

Analysis is performed in [R](https://www.r-project.org/), a free software environment for statistical computing and graphics, mainly using the [tidyverse](https://www.tidyverse.org/) packages, including [ggplot2](https://ggplot2.tidyverse.org/) for visualization. 


# text cleaning

Text cleaning involves five steps:

1. Extract 12 interviews from the transcript.
2. Replace [contractions](https://en.wikipedia.org/wiki/Contraction_(grammar)). For example: *"It's a fascinating field"* becomes *"It is a fascinating field"*.
3. Split sentences into single words. 
4. [Lemmatize](https://en.wikipedia.org/wiki/Lemmatisation) words. 
5. Remove [stop words](https://en.wikipedia.org/wiki/Stop_words), the most common[^1] words in English language.

Here is a quote from Jeffrey Shaffer's interview to illustrate steps 3-5:

```{r warning=FALSE, message=FALSE}
library(tidyverse)
library(tidytext)
sentense <- "I learned R and started doing visualizations in R"

example <- tibble(sentense) %>% 
  unnest_tokens(output = "word", input = sentense) %>% 
  mutate(word_lemma = textstem::lemmatize_words(word),
         stop_word = word %in% tm::stopwords(kind = "SMART"))

example
```



```{r echo = FALSE}

# Packages.
library(tidyverse)

# Colours.
dark_grey <- "grey30"
light_grey <- "grey80"
swd_light_blue <- "#95b3d7"
swd_dark_blue <- "#4f81bd"
swd_orange <- "#4f81bd" # this is now dark blue; used to be #f79646.

# Text import ----

raw_text <- readLines("swd_podcast_episode_14_learning_dataviz.txt", encoding = "ansi")

# Text cleaning ----

# 1. Extract the monologues of the 12 podcast guests from the transcript.

toc <- raw_text[11:25]
names(toc) <- str_replace_all(toc, pattern = "(.*)\\|(.*)", replacement = "\\2") %>% str_trim()

content <- raw_text[29:length(raw_text)]
content <- content[content != ""]

toc_lines <- purrr::map_int(.x = toc, .f = function(x) str_which(string = content, pattern = fixed(x)))
toc_lines <- tibble(interview = names(toc_lines),
                    interview_index = toc_lines)

# First and last paragraph of each interview is intro and outro of the host.
paragraphs_to_ignore <- sapply(c(-1, 0, 1), function(x) toc_lines$interview_index + x) %>% as.vector() %>% sort()

content_df <- tibble(paragraph_id = 1:length(content),
                     paragraph = content) %>% 
  left_join(toc_lines, by = c("paragraph_id" = "interview_index")) %>% 
  fill(interview, .direction = "down") %>% 
  mutate(ignore = interview %in% c("Intro", "Summary", "Updates") | paragraph_id %in% paragraphs_to_ignore) %>% 
  filter(!ignore)


# 2. Replace contractions.

# Before applying `qdap::replace_contractions()` we need to replace apostrophes from Windows-1252 code page (146) to a ASCII (39).
# Otherwise, the function may not work (did not work on one of two computers I tried).

content_df$paragraph <- str_replace_all(string = content_df$paragraph,
                                        pattern = gtools::chr(146), # https://en.wikipedia.org/wiki/Windows-1252
                                        replacement = gtools::chr(39))

content_df$paragraph <- qdap::replace_contraction(content_df$paragraph)

# 3., 4., 5. Tokenize, lemmatize, mark stop words.

# Define stop words.

# Know your stop words! For example, the SMART set (as documented in Appendix 11
# of http://jmlr.csail.mit.edu/papers/volume5/lewis04a/) has all single letters
# on English alphabet, including "r", which in the text may mean a programming
# language R and in this case you probably don't want to consider it as a stop
# word.

names_of_guests <- str_to_lower(names(toc)) %>% str_split(pattern = "\\s") %>% unlist()
stopwords_vec <- setdiff(c(tm::stopwords(kind = "SMART"), names_of_guests, "cole"), "r")

# Tokenize by words, lemmatize and mark stop words.
tokens <- content_df %>% 
  tidytext::unnest_tokens(output = "word", input = paragraph, token = "words") %>% 
  mutate(word_lemma = textstem::lemmatize_words(word),
         stop_word = word_lemma %in% stopwords_vec)

# Transform some words back after lemmatization.
tokens <- tokens %>% 
  mutate(word_lemma = case_when(word_lemma == "numb" ~ "number", 
                                word_lemma == "datum" ~ "data",
                                word_lemma == "infographics" ~ "infographic",
                                TRUE ~ word_lemma))

```


```{r echo=FALSE}
# Calculate word frequency.
tokens_freq <- tokens %>% 
  group_by(interview, stop_word, word_lemma) %>% 
  summarise(n = n()) %>% 
  ungroup()

stop_df <- tokens_freq %>% 
  group_by(interview, stop_word) %>% 
  summarize(n_stop_words = sum(n)) %>% 
  ungroup()

stop_df <- stop_df %>% 
  filter(stop_word) %>% 
  select(-stop_word)
  
tokens_freq_groups <- tokens_freq %>% 
  filter(!stop_word) %>% 
  group_by(interview) %>% 
  summarize(n_nonstop_words = sum(n),
            n_distinct_words = n_distinct(word_lemma)) %>% 
  mutate(n_repetitive_words = n_nonstop_words - n_distinct_words) %>% 
  left_join(stop_df, by = "interview") %>% 
  mutate(interview = reorder(interview, n_nonstop_words + n_stop_words)) %>% 
  select(-n_nonstop_words) %>% 
  gather(key, value, -interview) %>% 
  mutate(key = fct_rev(key))

# Remove stop words.
tokens_freq <- tokens_freq %>% 
  filter(!stop_word)
```


```{r echo = FALSE}
# Add TF-IDF.
tokens_tfidf <- tokens_freq %>% 
  tidytext::bind_tf_idf(word_lemma, interview, n)

zero_tfidf <- tokens_tfidf %>% 
  filter(tf_idf == 0) %>% 
  pull(word_lemma) %>% 
  unique()

zero_tfidf <- paste(zero_tfidf, sep = ",")
```





# about tf-idf {#about-tf-idf}

If you are already familiar with tf (term frequency), idf (inverse document frequency) and tf-idf (term frequency-inverse document frequency) then you may skip this section and go directly to the [visual comparison of these two measures of word importance](#tfidf).



Few definitions. Here, 

* *term* is a single word (in general, a term may be a combination of two words, a sentence, etc.)
* *document* is an interview - any single interview from the "learning dataviz" episode. 
* *collection of documents* is a collection of 12 interviews in the episode.

**Term frequency (tf)** measures how frequently a term occurs in a document. Since every document is different in length, it is often divided by the document length[^2]: 

$$\text{tf}(t) = \frac{\text{Number of times term t appears in a document}}{\text{Total number of terms in the document}}.$$

```{r echo=FALSE}
andy_one_word <- tokens_tfidf %>% 
  filter(interview == "Andy Cotgreave" & word_lemma == "learn")
```

For example, if the word "learn" is used 8 times and the interview contains 540 words then

$$\text{tf}(learn)=8/540\approx0.015.$$

When used to measure word importance, the shortcoming of tf is that it is calculated using a single document in isolation from other documents in the collection. Notice that a word's tf may be different for each document within a collection.

**Inverse document frequency (idf)** measures how important a term is in the context of other documents. Idf is computed as the logarithm of the number of the documents in the collection divided by the number of documents where the specific term appears:

$$\text{idf}(t) = log_e\left(\frac{\text{Total number of documents}}{\text{Number of documents with term t in it}}\right).$$

Here is the relationship between a number of documents with term $t$ in it and the term's idf. I assume there are 12 documents in the collection:

```{r echo = FALSE, fig.height=2.5, fig.width=5, fig.align='center'}
tibble(n_docs_with_term = 1:12,
       idf = log(12 / n_docs_with_term)) %>% 
  ggplot(aes(x = n_docs_with_term, y = idf)) +
  geom_point(size = 2) +
  geom_line() +
  labs(x = "Number of documents with term t in it",
       y = "idf(t)") +
  scale_y_continuous(breaks = seq(0, 3, by = 0.5), minor_breaks = NULL, expand = expand_scale(add = 0.05)) +
  scale_x_continuous(breaks = 1:12) +
  theme_classic() +
  theme(panel.grid = element_blank(), 
        axis.text.x = element_text(size = 10, colour = "grey30"),
        axis.text.y = element_text(size = 10, colour = "grey30"), 
        axis.title = element_text(size = 10, colour = "grey30"), 
        plot.title = element_text(size = 11, colour = "grey30", hjust = 0))
```



The "learning dataviz" episode of #SWDpodcast is, well, about learning data visualization. Naturally, the word "learn" appears at least once in each of 12 interviews. Does it mean that this word is important? In describing the whole episode - yes. In describing any particular interview from this episode - no, at least in the understanding of idf. Because *all* interviewees use the word "learn". 

The word "learn" appears in all 12 interviews, so its idf is zero: 

$$\text{idf}(learn)=log_e(12/12)=log_e(1)=0$$

It means that the word "learn" is completely unimportant to describe any specific interview in the context of other 11 interviews. Other words with zero importance here are *`r setdiff(zero_tfidf, "learn")`*.

Notice that idf of each term is the same for all documents in a collection.



**Term frequency-inverse document frequency (tf-idf)** is a multiplication of tf and idf:

$$\text{tfidf}(t) = \text{tf}(t) \times \text{idf}(t).$$

It does not matter how high $\text{tf}(learn)$ is, if $\text{idf}(learn)=0$, the $\text{tfidf}(learn)$ will also be zero. On the contrary, if a word is mentioned only in one interview from 12 then its tf gets multiplied by `r round(log(12/1), 2)`. This is what happened to the word "copy". Its importance rank increased from #40, measured by tf, to #3, measured by tf-idf.


Tf-idf gives highest weight to words that are "common locally and rare globally"[^3]. "Common locally" refers to the tf component, while "rare globally" refers to the idf component.



# tf-idf: the effect of idf on tf visualised {#tfidf}

The plot below depicts changes in *ranks* (position relative to other words) for the most important words in Andy Cotgreave's interview in the "learning dataviz" episode of #SWDpodast measured by tf (on the left) and tf-idf (on the right). Tf gives the highest weight to words that are simply used most frequently. It is a local measure in a sense that it ignores other interviews in the same podcast episode. 

Multiplication of tf by idf, which gives tf-idf, diminished the importance of words that are common across all interviews, such as "data" and "learn". At the same time it amplifies the importance of rare words, such as "copy" and "fun". 

A word "copy" is used only in Andy's interview. It is used in the context of recommending Austin Kleon's book "Steal Like an Artist". A word "fun" is used only in two more interviews. Andy recommends listeners to *"get out of your comfort zone and have fun"*. And continues with saying *"Have fun. My gosh, you are allowed to have fun!"*. Let's do it!


```{r message=FALSE, warning=FALSE, echo = FALSE, fig.width=6, fig.height=6.2, fig.align='center'}
library(ggrepel)

swd_light_blue <- "#95b3d7"
swd_dark_blue <- "#4f81bd"

# Source for `reverselog_trans()`: https://stackoverflow.com/questions/11053899/how-to-get-a-reversed-log10-scale-in-ggplot2
library("scales")
reverselog_trans <- function(base = exp(1)) {
  trans <- function(x) -log(x, base)
  inv <- function(x) base^(-x)
  trans_new(paste0("reverselog-", format(base)), trans, inv, 
            log_breaks(base = base), 
            domain = c(1e-100, Inf))
}

slopes_tf_tfidf <- tokens_tfidf %>% 
  filter(interview == "Andy Cotgreave") %>% 
  add_count(name = "total_words", wt = n) %>% 
  # arrange(desc(word_lemma)) %>% 
  mutate(word_tf_rank = row_number(-tf),
         word_tfidf_rank = row_number(-tf_idf)) %>%
  filter(word_tf_rank %in% 1:6 | word_tfidf_rank %in% 1:6) %>% 
  ungroup()

slopes_tf_tfidf_gathered <- slopes_tf_tfidf %>% 
  select(interview, word_lemma, word_tf_rank, word_tfidf_rank) %>% 
  mutate(rank_inc = word_tf_rank > word_tfidf_rank) %>% 
  gather(key, value, word_tf_rank, word_tfidf_rank)

gg_title <- 'tf-idf: the effect of idf on tf'
gg_subtitle <- "Comparison of two measures of word importance\nRanks are positions relative to other words\nHighest rank means highest importance"
gg_caption <- "Source: transcript of #SWD podcast episode 'learning dataviz'.\nIllustration by @DmitrijsKass in R with ggplot2."

set.seed(1)
slopes_tf_tfidf_gathered %>% 
  ggplot(aes(x = key, y = value, group = word_lemma, label = word_lemma)) +
  geom_point(colour = swd_light_blue, size = 2) +
  geom_point(data = filter(slopes_tf_tfidf_gathered, rank_inc == TRUE), colour = swd_dark_blue, size = 2) +
  geom_line(colour = swd_light_blue) +
  geom_line(data = filter(slopes_tf_tfidf_gathered, rank_inc == TRUE), colour = swd_dark_blue, size = 1.05) +
  geom_text_repel(data = filter(slopes_tf_tfidf_gathered, key == "word_tf_rank"), 
                  mapping = aes(color = rank_inc),
                  nudge_x = -0.02, 
                  hjust = "outward",
                  segment.colour = "grey20",
                  show.legend = FALSE) +
  geom_text_repel(data = filter(slopes_tf_tfidf_gathered, key != "word_tf_rank"), 
                  mapping = aes(color = rank_inc),
                  nudge_x = +0.02, 
                  hjust = "outward",
                  segment.colour = "grey20",
                  show.legend = FALSE) +
  scale_color_manual(values = c("TRUE" = swd_dark_blue, "FALSE" = swd_light_blue)) +
  scale_y_continuous(trans = reverselog_trans(10), 
                     sec.axis = dup_axis(name = str_wrap("rank of words by tf-idf", width = 8)), 
                     breaks = c(1:3, 5, 10, 50, 100, 300), 
                     minor_breaks = NULL) +
  scale_x_discrete(labels = NULL, 
                   position = "top", 
                   expand = expand_scale(mult = 0.25, add = 0)) +
  labs(x = NULL,
       y = str_wrap("rank of words by tf", width = 8),
       title = gg_title,
       subtitle = gg_subtitle,
       caption = gg_caption) +
  theme_minimal() +
  theme(panel.grid = element_blank(), 
        axis.title.y.left = element_text(angle = 0, colour = "grey30", vjust = 0.97),
        axis.title.y.right = element_text(angle = 0, colour = "grey30", vjust = 0.97),
        axis.text.x = element_text(size = 12, colour = "grey30"),
        axis.text.y = element_text(size = 10, colour = "grey30"), 
        plot.title = element_text(size = 15, colour = "grey30", hjust = 0.5, margin = margin(b = 0)),
        plot.subtitle = element_text(size = 13, colour = "grey50", hjust = 0.5, margin = margin(b = 15, t = 5)),
        plot.caption = element_text(size = 8, colour = "grey40", hjust = 0.5, margin = margin(t = 15)))
```

Below is the data used for plotting. There are 12 interviews. Andy Cotgreave's interview contains 540 words, 271 of them unique. Importance ranks are positions relative to other words.

```{r echo = FALSE}
terms_in_documents <- tokens_tfidf %>% 
  group_by(word_lemma) %>% 
  summarise(documents = n_distinct(interview))

t <- slopes_tf_tfidf %>% 
  left_join(terms_in_documents, by = "word_lemma") %>% 
  select(word_lemma, freq = n, total_words, tf, documents, idf, tf_idf, tf_rank = word_tf_rank, tfidf_rank = word_tfidf_rank) %>% 
  arrange(desc(freq), desc(tfidf_rank))

cols <- c("word", "count", "total words", "tf", "documents with word", "idf", "tf-idf", "rank by tf", "rank by idf")
knitr::kable(t, format = "html", col.names = cols, digits = 3)
```



<br>

**References:**

* Learn about text analysis in a tidy way with [Text Mining with R](https://www.tidytextmining.com/)
* R code for this blog post is available at my [GitHub repository](https://github.com/dmitrijsk/blogdown-dmitrijskass/blob/redo-swd-artisanal-data/content/post/2019-05-08-swdchallenge-artisanal-data.R).



<br>

Any comments or suggestions? I'd be glad to know! Please leave them below, no login required if you check "I'd rather post as a guest".




[^1]: Stop words from the [SMART information retrieval system](http://www.jmlr.org/papers/volume5/lewis04a/lewis04a.pdf) available in `tm` package served as the basis. They were supplemented with the names of podcast host and guests. One stop word was removed, it is a letter *"r"*, which is used by one of the podcast guests to refer to R as a software. It is definitely worth knowing prepackaged stop words!
[^2]: http://www.tfidf.com
[^3]: I heard the phrase "common locally and rare globally" for the first time [here](https://www.coursera.org/lecture/ml-foundations/calculating-tf-idf-vectors-1rg5n) and I liked this definition a lot.
