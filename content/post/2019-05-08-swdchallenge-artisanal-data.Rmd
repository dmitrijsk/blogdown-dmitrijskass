---
title: '#SWDchallenge: artisanal data'
author: Dmitrijs Kass
date: '2019-05-10'
categories:
  - data-viz
  - text analysis
tags:
  - SWDchallenge
slug: swdchallenge-artisanal-data
output:
  blogdown::html_page:
    toc: yes
---

This post is a participation in [#SWDchallenge: artisanal data](http://www.storytellingwithdata.com/blog/2019/5/1/swdchallenge-artisanal-data).

I use the [transcript](https://docs.google.com/document/d/1S2_63MUbvQs7fxWQrcuCNSl03fmDl1Vr3FjNjnbWK14/edit#) of the "learning dataviz" episode of [#SWDpodcast](http://www.storytellingwithdata.com/podcast), where 12 data visualization professionals share their stories and recommendations.

I find a lot of wisdom and inspiration in this episode. [Storytelling with Data](http://www.storytellingwithdata.com/book) (SWD) book has been on my shelf for over a year and it is this podcast episode that sparkled interest in me about data visualization and made me read the book in few days and start participating in data viz challenges. So I thought it would enjoy spending more time with this episode by analysing the words in it.

The goal of the analysis is to visualize how importance of words changes when measured by **tf** (**term frequency**) versus  [**tf-idf** (**term frequency-inverse document frequency**)](http://www.tfidf.com/). The latter gives higher weight to words that are "common locally and rare globally"[^1]. Here, locally means "in any single interview" and globally means "in a collection of all 12 interviews".

Analysis is performed in [R](https://www.r-project.org/), a free software environment for statistical computing and graphics, using the tools mainly from the [tidyverse](https://www.tidyverse.org/) packages, including [ggplot2](https://ggplot2.tidyverse.org/) for visualization. 


# text cleaning

Text cleaning involves five steps:

1. Extract 12 interviews from the transcript.
2. Replace [contractions](https://en.wikipedia.org/wiki/Contraction_(grammar)). For example: *"It's a fascinating field"* becomes *"It is a fascinating field"*.
3. Split sentences into single words. 
4. [Lemmatize](https://en.wikipedia.org/wiki/Lemmatisation) words. 
5. Remove [stop words](https://en.wikipedia.org/wiki/Stop_words), the most common[^2] words in English language.

Here is a quote from Jeffrey Shaffer's interview to illustrate steps 3-5:

```{r warning=FALSE, message=FALSE}
library(tidyverse)
library(tidytext)
sentense <- "I learned R and started doing visualizations in R"

example <- tibble(sentense) %>% 
  unnest_tokens(output = "word", input = sentense) %>% 
  mutate(word_lemma = textstem::lemmatize_words(word),
         stop_word = word %in% tm::stopwords(kind = "SMART"))

example
```


# tf-idf: the effect of idf on tf

```{r echo = FALSE}

# Packages.
library(tidyverse)

# Colours.
dark_grey <- "grey30"
light_grey <- "grey80"
swd_light_blue <- "#95b3d7"
swd_dark_blue <- "#4f81bd"
swd_orange <- "#4f81bd" # this is now dark blue; used to be #f79646.

# Text import ----

raw_text <- readLines("swd_podcast_episode_14_learning_dataviz.txt", encoding = "ansi")

# Text cleaning ----

# 1. Extract the monologues of the 12 podcast guests from the transcript.

toc <- raw_text[11:25]
names(toc) <- str_replace_all(toc, pattern = "(.*)\\|(.*)", replacement = "\\2") %>% str_trim()

content <- raw_text[29:length(raw_text)]
content <- content[content != ""]

toc_lines <- purrr::map_int(.x = toc, .f = function(x) str_which(string = content, pattern = fixed(x)))
toc_lines <- tibble(interview = names(toc_lines),
                    interview_index = toc_lines)

# First and last paragraph of each interview is intro and outro of the host.
paragraphs_to_ignore <- sapply(c(-1, 0, 1), function(x) toc_lines$interview_index + x) %>% as.vector() %>% sort()

content_df <- tibble(paragraph_id = 1:length(content),
                     paragraph = content) %>% 
  left_join(toc_lines, by = c("paragraph_id" = "interview_index")) %>% 
  fill(interview, .direction = "down") %>% 
  mutate(ignore = interview %in% c("Intro", "Summary", "Updates") | paragraph_id %in% paragraphs_to_ignore) %>% 
  filter(!ignore)


# 2. Replace contractions.

# Before applying `qdap::replace_contractions()` we need to replace apostrophes from Windows-1252 code page (146) to a ASCII (39).
# Otherwise, the function may not work (did not work on one of two computers I tried).

content_df$paragraph <- str_replace_all(string = content_df$paragraph,
                                        pattern = gtools::chr(146), # https://en.wikipedia.org/wiki/Windows-1252
                                        replacement = gtools::chr(39))

content_df$paragraph <- qdap::replace_contraction(content_df$paragraph)

# 3., 4., 5. Tokenize, lemmatize, mark stop words.

# Define stop words.

# Know your stop words! For example, the SMART set (as documented in Appendix 11
# of http://jmlr.csail.mit.edu/papers/volume5/lewis04a/) has all single letters
# on English alphabet, including "r", which in the text may mean a programming
# language R and in this case you probably don't want to consider it as a stop
# word.

names_of_guests <- str_to_lower(names(toc)) %>% str_split(pattern = "\\s") %>% unlist()
stopwords_vec <- setdiff(c(tm::stopwords(kind = "SMART"), names_of_guests, "cole"), "r")

# Tokenize by words, lemmatize and mark stop words.
tokens <- content_df %>% 
  tidytext::unnest_tokens(output = "word", input = paragraph, token = "words") %>% 
  mutate(word_lemma = textstem::lemmatize_words(word),
         stop_word = word_lemma %in% stopwords_vec)

# Transform some words back after lemmatization.
tokens <- tokens %>% 
  mutate(word_lemma = case_when(word_lemma == "numb" ~ "number", 
                                word_lemma == "datum" ~ "data",
                                word_lemma == "infographics" ~ "infographic",
                                TRUE ~ word_lemma))

```


```{r echo=FALSE}
# Calculate word frequency.
tokens_freq <- tokens %>% 
  group_by(interview, stop_word, word_lemma) %>% 
  summarise(n = n()) %>% 
  ungroup()

stop_df <- tokens_freq %>% 
  group_by(interview, stop_word) %>% 
  summarize(n_stop_words = sum(n)) %>% 
  ungroup()

stop_df <- stop_df %>% 
  filter(stop_word) %>% 
  select(-stop_word)
  
tokens_freq_groups <- tokens_freq %>% 
  filter(!stop_word) %>% 
  group_by(interview) %>% 
  summarize(n_nonstop_words = sum(n),
            n_distinct_words = n_distinct(word_lemma)) %>% 
  mutate(n_repetitive_words = n_nonstop_words - n_distinct_words) %>% 
  left_join(stop_df, by = "interview") %>% 
  mutate(interview = reorder(interview, n_nonstop_words + n_stop_words)) %>% 
  select(-n_nonstop_words) %>% 
  gather(key, value, -interview) %>% 
  mutate(key = fct_rev(key))

# Remove stop words.
tokens_freq <- tokens_freq %>% 
  filter(!stop_word)
```


```{r echo = FALSE}
# Add TF-IDF.
tokens_tfidf <- tokens_freq %>% 
  tidytext::bind_tf_idf(word_lemma, interview, n) %>% 
  # Arrange by facet and word frequency.
  arrange(interview, tf_idf) %>% 
  # Add order column of row numbers
  mutate(order = row_number())

zero_tfidf <- tokens_tfidf %>% 
  filter(tf_idf == 0) %>% 
  pull(word_lemma) %>% 
  unique()

zero_tfidf <- paste(zero_tfidf, sep = ",")
```

If you are not familiar with terms tf (term frequency) and idf (inverse document frequency) then they are described below. Otherwise, here is the illustration:

```{r message=FALSE, warning=FALSE, echo = FALSE, fig.width=5, fig.height=7.5, fig.align='center'}
library(ggrepel)

swd_light_blue <- "#95b3d7"
swd_dark_blue <- "#4f81bd"

# Source for `reverselog_trans()`: https://stackoverflow.com/questions/11053899/how-to-get-a-reversed-log10-scale-in-ggplot2
library("scales")
reverselog_trans <- function(base = exp(1)) {
  trans <- function(x) -log(x, base)
  inv <- function(x) base^(-x)
  trans_new(paste0("reverselog-", format(base)), trans, inv, 
            log_breaks(base = base), 
            domain = c(1e-100, Inf))
}


slopes_tf_tfidf <- tokens_tfidf %>% 
  filter(interview == "Andy Cotgreave") %>% 
  mutate(word_tf_rank = row_number(-tf),
         word_tfidf_rank = row_number(-tf_idf)) %>%
  filter(word_tf_rank %in% 1:6 | word_tfidf_rank %in% 1:6) %>% 
  ungroup() %>% 
  select(interview, word_lemma, word_tf_rank, word_tfidf_rank) %>% 
  mutate(rank_inc = word_tf_rank > word_tfidf_rank) %>% 
  gather(key, value, word_tf_rank, word_tfidf_rank)

gg_title <- 'tf-idf: the effect of idf on tf.'
gg_subtitle <- paste(str_wrap(
  "As an example, these are some of words used by Andy Cotgreave in the 'learning dataviz' episode of #SWDpodast. Although words such as 'data' and 'learn' are the 1st and the 5th most frequent, they are not important when compared to other 11 interviews on the same podcast episode. This is because all 12 interviews are about learning data visualization.", width = 77),
  "",
  str_wrap("Notice how idf diminishes the weights of words that appear in many interviews (e.g. 'data', 'learn') and promotes other words (e.g. 'copy', 'fun') as more relevant for a particular interview.", width = 77),
  "",
  str_wrap("A word 'copy' is important in the context of recommending Austin Kleon's book Steal Like an Artist. A word 'fun' is important in recommending the listeners to 'get out of your comfort zone and have fun'. 'Have fun. My gosh, you are allowed to have fun!' invites Andy.", width = 77),
  sep = "\n")

gg_caption <- "Source: transcript of #SWD podcast episode 'learning dataviz'.\nIllustration by @DmitrijsKass in R with ggplot2."

set.seed(3)
slopes_tf_tfidf %>% 
  ggplot(aes(x = key, y = value, group = word_lemma, label = word_lemma)) +
  geom_point(colour = swd_light_blue, size = 2) +
  geom_point(data = filter(slopes_tf_tfidf, rank_inc == TRUE), colour = swd_dark_blue, size = 2) +
  geom_line(colour = swd_light_blue) +
  geom_line(data = filter(slopes_tf_tfidf, rank_inc == TRUE), colour = swd_dark_blue, size = 1.05) +
  geom_text_repel(data = filter(slopes_tf_tfidf, key == "word_tf_rank"), 
                  mapping = aes(color = rank_inc),
                  nudge_x = -0.02, 
                  hjust = "outward",
                  show.legend = FALSE) +
  geom_text_repel(data = filter(slopes_tf_tfidf, key != "word_tf_rank"), 
                  mapping = aes(color = rank_inc),
                  nudge_x = +0.02, 
                  hjust = "outward",
                  show.legend = FALSE) +
  scale_color_manual(values = c("TRUE" = swd_dark_blue, "FALSE" = swd_light_blue)) +
  scale_y_continuous(trans = reverselog_trans(10), 
                     sec.axis = dup_axis(), 
                     breaks = c(1:3, 5, 10, 50, 100, 300), 
                     minor_breaks = NULL) +
  scale_x_discrete(labels = c("rank of words\nby tf\n", "rank of words\nby tf-idf\n"), 
                   position = "top", 
                   expand = expand_scale(mult = 0.2, add = 0)) +
  labs(x = NULL,
       y = NULL,
       title = gg_title,
       subtitle = gg_subtitle,
       caption = gg_caption) +
  theme_minimal() +
  theme(panel.grid = element_blank(), 
        axis.text.x = element_text(size = 12, colour = "grey30"),
        axis.text.y = element_text(size = 10, colour = "grey30"), 
        plot.title = element_text(size = 15, colour = "grey30", hjust = 0.5),
        plot.subtitle = element_text(size = 9.5, colour = "grey40", margin = margin(b = 15, t = 15)),
        plot.caption = element_text(size = 8, colour = "grey40", hjust = 0.5, margin = margin(t = 15)))
```

# about tf-idf

Few definitions. Here, the *term* means a single word. In general, a term may be a combination of two words, a sentence, etc. A *document* is an interview - any single interview from the "learning dataviz" episode. A *collection of documents* is a collection of all 12 interviews in the same episode.

**Term frequency (tf)** measures how frequently a term occurs in a document. Since every document is different in length, it is often divided by the document length: 

$$tf(t) = \frac{\text{Number of times term t appears in a document}}{\text{Total number of terms in the document}}.$$
The shortcoming of tf is that it is calculated using each single document in isolation from other documents in the collection. 

**Inverse document frequency (idf)** measures how important a term is. Even if stop words were removed, high term frequency does not obligatory mean that the term is important. Idf is computed as the logarithm of the number of the documents in the collection divided by the number of documents where the specific term appears:

$$idf(t) = log_e\left(\frac{\text{Total number of documents}}{\text{Number of documents with term t in it}}\right).$$

For example, the "learning dataviz" episode of #SWDpodcast is, well, about learning data visualization. Naturally, words "data" and "learn" appear frequently in each of 12 interviews. Does it mean that these words are important? In describing the podcast episode - yes. In describing any particular interview from this episode - no, because all interviewees use these two words.

Mathematically, if the word "learn" appears in all 12 interviews then its idf is zero: 

$$log_e(12/12)=log_e(1)=0$$

Other words that have zero idf in this analysis are *`r zero_tfidf`*.

**Term frequency-inverse document frequency (tf-idf)** is a multiplication of tf and idf. 

Notice that the value of a term's tf is local (document-specific), while its idf is global (single value within a collection of documents). For example, **tf** of the word *"learn"* in one interview is 0.01, in another it is 0.02. Its **idf** is constant within the collection of interviews - zero. Therefore, no matter how often it appears in an any single interview, its **tf-idf** is always zero. And it has the lowest rank in the list of important words, measured by tf-idf - exactly as in the plot above.

<br>

**References:**

* Learn about text analysis in a tidy way with [Text Mining with R](https://www.tidytextmining.com/)
* R code for this blog post is available at my [GitHub repository](https://github.com/dmitrijsk/blogdown-dmitrijskass/blob/redo-swd-artisanal-data/content/post/2019-05-08-swdchallenge-artisanal-data.R).



<br>

Did you find this post interesting or useful? If not, I'd be glad to improve it. Please leave a comment below, no login required if you check "I'd rather post as a guest".




[^1]: I heard the phrase "common locally and rare globally" for the first time [here](https://www.coursera.org/lecture/ml-foundations/calculating-tf-idf-vectors-1rg5n) and I liked this definition a lot.
[^2]: Stop words from the [SMART information retrieval system](http://www.jmlr.org/papers/volume5/lewis04a/lewis04a.pdf) available in `tm` package served as the basis. They were supplemented with the names of podcast host and guests. One stop word was removed, it is a letter *"r"*, which is used by one of the podcast guests to refer to R as a software. It is definitely worth knowing prepackaged stop words!
