---
title: '#SWDchallenge: artisanal data'
author: Dmitrijs Kass
date: '2019-05-10'
categories:
  - data-viz
  - text analysis
tags:
  - SWDchallenge
slug: swdchallenge-artisanal-data
output:
  blogdown::html_page:
    toc: yes
---


<div id="TOC">
<ul>
<li><a href="#text-cleaning">text cleaning</a></li>
<li><a href="#tf-idf-the-effect-of-idf-on-tf">tf-idf: the effect of idf on tf</a></li>
<li><a href="#about-tf-idf">about tf-idf</a></li>
</ul>
</div>

<p>This post is a participation in <a href="http://www.storytellingwithdata.com/blog/2019/5/1/swdchallenge-artisanal-data">#SWDchallenge: artisanal data</a>.</p>
<p>I use the <a href="https://docs.google.com/document/d/1S2_63MUbvQs7fxWQrcuCNSl03fmDl1Vr3FjNjnbWK14/edit#">transcript</a> of the “learning dataviz” episode of <a href="http://www.storytellingwithdata.com/podcast">#SWDpodcast</a>, where 12 data visualization professionals share their stories and recommendations.</p>
<p>I find a lot of wisdom and inspiration in this episode. <a href="http://www.storytellingwithdata.com/book">Storytelling with Data</a> (SWD) book has been on my shelf for over a year and it is this podcast episode that sparkled interest in me about data visualization and made me read the book in few days and start participating in data viz challenges. So I thought it would enjoy spending more time with this episode by analysing the words in it.</p>
<p>The goal of the analysis is to visualize how importance of words changes when measured by <strong>tf</strong> (<strong>term frequency</strong>) versus <a href="http://www.tfidf.com/"><strong>tf-idf</strong> (<strong>term frequency-inverse document frequency</strong>)</a>. The latter gives higher weight to words that are “common locally and rare globally”<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>. Here, locally means “in any single interview” and globally means “in a collection of all 12 interviews”.</p>
<p>Analysis is performed in <a href="https://www.r-project.org/">R</a>, a free software environment for statistical computing and graphics, using the tools mainly from the <a href="https://www.tidyverse.org/">tidyverse</a> packages, including <a href="https://ggplot2.tidyverse.org/">ggplot2</a> for visualization.</p>
<div id="text-cleaning" class="section level1">
<h1>text cleaning</h1>
<p>Text cleaning involves five steps:</p>
<ol style="list-style-type: decimal">
<li>Extract 12 interviews from the transcript.</li>
<li>Replace <a href="https://en.wikipedia.org/wiki/Contraction_(grammar)">contractions</a>. For example: <em>“It’s a fascinating field”</em> becomes <em>“It is a fascinating field”</em>.</li>
<li>Split sentences into single words.</li>
<li><a href="https://en.wikipedia.org/wiki/Lemmatisation">Lemmatize</a> words.</li>
<li>Remove <a href="https://en.wikipedia.org/wiki/Stop_words">stop words</a>, the most common<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> words in English language.</li>
</ol>
<p>Here is a quote from Jeffrey Shaffer’s interview to illustrate steps 3-5:</p>
<pre class="r"><code>library(tidyverse)
library(tidytext)
sentense &lt;- &quot;I learned R and started doing visualizations in R&quot;

example &lt;- tibble(sentense) %&gt;% 
  unnest_tokens(output = &quot;word&quot;, input = sentense) %&gt;% 
  mutate(word_lemma = textstem::lemmatize_words(word),
         stop_word = word %in% tm::stopwords(kind = &quot;SMART&quot;))

example</code></pre>
<pre><code>## # A tibble: 9 x 3
##   word           word_lemma    stop_word
##   &lt;chr&gt;          &lt;chr&gt;         &lt;lgl&gt;    
## 1 i              i             TRUE     
## 2 learned        learn         FALSE    
## 3 r              r             TRUE     
## 4 and            and           TRUE     
## 5 started        start         FALSE    
## 6 doing          do            TRUE     
## 7 visualizations visualization FALSE    
## 8 in             in            TRUE     
## 9 r              r             TRUE</code></pre>
</div>
<div id="tf-idf-the-effect-of-idf-on-tf" class="section level1">
<h1>tf-idf: the effect of idf on tf</h1>
<p>If you are not familiar with terms tf (term frequency) and idf (inverse document frequency) then they are described below. Otherwise, here is the illustration:</p>
<p><img src="/post/2019-05-08-swdchallenge-artisanal-data_files/figure-html/unnamed-chunk-5-1.png" width="480" style="display: block; margin: auto;" /></p>
</div>
<div id="about-tf-idf" class="section level1">
<h1>about tf-idf</h1>
<p>Few definitions. Here, the <em>term</em> means a single word. In general, a term may be a combination of two words, a sentence, etc. A <em>document</em> is an interview - any single interview from the “learning dataviz” episode. A <em>collection of documents</em> is a collection of all 12 interviews in the same episode.</p>
<p><strong>Term frequency (tf)</strong> measures how frequently a term occurs in a document. Since every document is different in length, it is often divided by the document length:</p>
<p><span class="math display">\[tf(t) = \frac{\text{Number of times term t appears in a document}}{\text{Total number of terms in the document}}.\]</span>
The shortcoming of tf is that it is calculated using each single document in isolation from other documents in the collection.</p>
<p><strong>Inverse document frequency (idf)</strong> measures how important a term is. Even if stop words were removed, high term frequency does not obligatory mean that the term is important. Idf is computed as the logarithm of the number of the documents in the collection divided by the number of documents where the specific term appears:</p>
<p><span class="math display">\[idf(t) = log_e\left(\frac{\text{Total number of documents}}{\text{Number of documents with term t in it}}\right).\]</span></p>
<p>For example, the “learning dataviz” episode of #SWDpodcast is, well, about learning data visualization. Naturally, words “data” and “learn” appear frequently in each of 12 interviews. Does it mean that these words are important? In describing the podcast episode - yes. In describing any particular interview from this episode - no, because all interviewees use these two words.</p>
<p>Mathematically, if the word “learn” appears in all 12 interviews then its idf is zero:</p>
<p><span class="math display">\[log_e(12/12)=log_e(1)=0\]</span></p>
<p>Other words that have zero idf in this analysis are <em>data, year, learn, make, thing, work</em>.</p>
<p><strong>Term frequency-inverse document frequency (tf-idf)</strong> is a multiplication of tf and idf.</p>
<p>Notice that the value of a term’s tf is local (document-specific), while its idf is global (single value within a collection of documents). For example, <strong>tf</strong> of the word <em>“learn”</em> in one interview is 0.01, in another it is 0.02. Its <strong>idf</strong> is constant within the collection of interviews - zero. Therefore, no matter how often it appears in an any single interview, its <strong>tf-idf</strong> is always zero. Ant it has the lowest rank in the list of important words, measured by tf-idf - exactly as in the plot above.</p>
<p><br></p>
<p><strong>References:</strong></p>
<ul>
<li>Learn about text analysis in a tidy way with <a href="https://www.tidytextmining.com/">Text Mining with R</a></li>
<li>R code for this blog post is available at my <a href="">GitHub repository</a>.</li>
</ul>
<p><br></p>
<p>Did you find this post interesting or useful? If not, I’d be glad to improve it. Please leave a comment below, no login required if you check “I’d rather post as a guest”.</p>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>I heard the phrase “common locally and rare globally” for the first time <a href="https://www.coursera.org/lecture/ml-foundations/calculating-tf-idf-vectors-1rg5n">here</a> and I liked this definition a lot.<a href="#fnref1" class="footnote-back">↩</a></p></li>
<li id="fn2"><p>Stop words from the <a href="http://www.jmlr.org/papers/volume5/lewis04a/lewis04a.pdf">SMART information retrieval system</a> available in <code>tm</code> package served as the basis. They were supplemented with the names of podcast host and guests. One stop word was removed, it is a letter <em>“r”</em>, which is used by one of the podcast guests to refer to R as a software. It is definitely worth knowing prepackaged stop words!<a href="#fnref2" class="footnote-back">↩</a></p></li>
</ol>
</div>
