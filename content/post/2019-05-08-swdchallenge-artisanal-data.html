---
title: '#SWDchallenge: artisanal data'
author: Dmitrijs Kass
date: '2019-05-10'
categories:
  - data-viz
  - text analysis
tags:
  - SWDchallenge
slug: swdchallenge-artisanal-data
output:
  blogdown::html_page:
    toc: yes
---

<script src="/rmarkdown-libs/kePrint/kePrint.js"></script>

<div id="TOC">
<ul>
<li><a href="#text-cleaning">text cleaning</a></li>
<li><a href="#tfidf">tf-idf: the effect of idf on tf</a></li>
<li><a href="#about-tf-idf">about tf-idf</a></li>
</ul>
</div>

<p>This post is a participation in <a href="http://www.storytellingwithdata.com/blog/2019/5/1/swdchallenge-artisanal-data">#SWDchallenge: artisanal data</a>. Along with that it visualizes the <strong>tf-idf - effect of multiplying tf by idf</strong>. Variations of tf-idf, term frequency-inverse document frequency, are often used by search engines and text-based recommender systems.</p>
<hr />
<p>I use the <a href="https://docs.google.com/document/d/1S2_63MUbvQs7fxWQrcuCNSl03fmDl1Vr3FjNjnbWK14/edit#">transcript</a> of the “learning dataviz” episode of <a href="http://www.storytellingwithdata.com/podcast">#SWDpodcast</a>, where 12 data visualization professionals share their stories and recommendations.</p>
<p>I find a lot of wisdom and inspiration in this episode. <a href="http://www.storytellingwithdata.com/book">Storytelling with Data</a> (SWD) book has been on my shelf for over a year and it is this podcast episode that sparkled interest in me about data visualization and made me read the book in few days and start participating in data viz challenges. So I thought I would enjoy spending more time with this episode by analysing the words in it.</p>
<p>The goal of the analysis is to visualize how importance of words changes when measured by <strong>tf</strong> (<strong>term frequency</strong>) versus <a href="http://www.tfidf.com/"><strong>tf-idf</strong> (<strong>term frequency-inverse document frequency</strong>)</a>. The latter gives higher weight to words that are “common locally and rare globally”<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>. Here, locally means “in any single interview” and globally means “in a collection of all 12 interviews”.</p>
<p>Analysis is performed in <a href="https://www.r-project.org/">R</a>, a free software environment for statistical computing and graphics, using the tools mainly from the <a href="https://www.tidyverse.org/">tidyverse</a> packages, including <a href="https://ggplot2.tidyverse.org/">ggplot2</a> for visualization.</p>
<div id="text-cleaning" class="section level1">
<h1>text cleaning</h1>
<p>Text cleaning involves five steps:</p>
<ol style="list-style-type: decimal">
<li>Extract 12 interviews from the transcript.</li>
<li>Replace <a href="https://en.wikipedia.org/wiki/Contraction_(grammar)">contractions</a>. For example: <em>“It’s a fascinating field”</em> becomes <em>“It is a fascinating field”</em>.</li>
<li>Split sentences into single words.</li>
<li><a href="https://en.wikipedia.org/wiki/Lemmatisation">Lemmatize</a> words.</li>
<li>Remove <a href="https://en.wikipedia.org/wiki/Stop_words">stop words</a>, the most common<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> words in English language.</li>
</ol>
<p>Here is a quote from Jeffrey Shaffer’s interview to illustrate steps 3-5:</p>
<pre class="r"><code>library(tidyverse)
library(tidytext)
sentense &lt;- &quot;I learned R and started doing visualizations in R&quot;

example &lt;- tibble(sentense) %&gt;% 
  unnest_tokens(output = &quot;word&quot;, input = sentense) %&gt;% 
  mutate(word_lemma = textstem::lemmatize_words(word),
         stop_word = word %in% tm::stopwords(kind = &quot;SMART&quot;))

example</code></pre>
<pre><code>## # A tibble: 9 x 3
##   word           word_lemma    stop_word
##   &lt;chr&gt;          &lt;chr&gt;         &lt;lgl&gt;    
## 1 i              i             TRUE     
## 2 learned        learn         FALSE    
## 3 r              r             TRUE     
## 4 and            and           TRUE     
## 5 started        start         FALSE    
## 6 doing          do            TRUE     
## 7 visualizations visualization FALSE    
## 8 in             in            TRUE     
## 9 r              r             TRUE</code></pre>
</div>
<div id="tfidf" class="section level1">
<h1>tf-idf: the effect of idf on tf</h1>
<p>If you are not familiar with tf (term frequency) and idf (inverse document frequency) then they are described <a href="#about-tf-idf">below</a> with examples. Otherwise, here is the illustration:</p>
<p><img src="/post/2019-05-08-swdchallenge-artisanal-data_files/figure-html/unnamed-chunk-5-1.png" width="480" style="display: block; margin: auto;" /></p>
</div>
<div id="about-tf-idf" class="section level1">
<h1>about tf-idf</h1>
<p>Few definitions. Here, the <em>term</em> means a single word. In general, a term may be a combination of two words, a sentence, etc. A <em>document</em> is an interview - any single interview from the “learning dataviz” episode. A <em>collection of documents</em> is a collection of all 12 interviews in the same episode.</p>
<p><strong>Term frequency (tf)</strong> measures how frequently a term occurs in a document. Since every document is different in length, it is often divided by the document length:</p>
<p><span class="math display">\[tf(t) = \frac{\text{Number of times term t appears in a document}}{\text{Total number of terms in the document}}.\]</span></p>
<p>For example, if the word “learn” is used 8 times and the interview contains 540 words then tf equals</p>
<p><span class="math display">\[tf(learn)=8/540\approx0.015.\]</span></p>
<p>The shortcoming of tf is that it is calculated using a single document in isolation from other documents in the collection.</p>
<p><strong>Inverse document frequency (idf)</strong> measures how important a term is. Even if stop words were removed, high term frequency does not obligatory mean that the term is important. Idf is computed as the logarithm of the number of the documents in the collection divided by the number of documents where the specific term appears:</p>
<p><span class="math display">\[idf(t) = log_e\left(\frac{\text{Total number of documents}}{\text{Number of documents with term t in it}}\right).\]</span></p>
<p>The “learning dataviz” episode of #SWDpodcast is, well, about learning data visualization. Naturally, the word “learn” appears at least once in each of 12 interviews. Does it mean that this word is important? In describing the podcast episode - yes. In describing any particular interview from this episode - no, because all interviewees use the word “learn”.</p>
<p>The word “learn” appears in all 12 interviews, so its idf is zero:</p>
<p><span class="math display">\[idf(learn)=log_e(12/12)=log_e(1)=0\]</span></p>
<p>It means that the word “learn” is not important in the context of 12 interviews. Other words that have zero importance here are <em>data, year, make, thing, work</em>.</p>
<p><strong>Term frequency-inverse document frequency (tf-idf)</strong> is a multiplication of tf and idf:</p>
<p><span class="math display">\[tfidf(t) = tf(t) \times idf(t).\]</span></p>
<p>It does not matter how high <span class="math inline">\(tf(learn)\)</span> is, if <span class="math inline">\(idf(learn)=0\)</span>, its <span class="math inline">\(tfidf(learn)\)</span> will also be zero.</p>
<p>Notice that the value of a term’s tf is <strong>local</strong> (document-specific), while its idf is <strong>global</strong> (single value within a collection of documents). For example, tf of the word “learn” in Andy’s interview is 0.015 and 0.001 Jeffrey’s interview. Its idf is constant within the collection of interviews - zero. Therefore, no matter how often it appears in an any single interview, its tf-idf is always zero. And it has the lowest rank in the list of important words, measured by tf-idf - exactly as in the plot <a href="#tfidf">above</a>.</p>
<p>Here is the data used for plotting - top an bottom six most important words in Andy Cotgreave’s interview:</p>
<table class="table table-striped table-condensed" style="margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
word
</th>
<th style="text-align:right;">
count
</th>
<th style="text-align:right;">
total words
</th>
<th style="text-align:right;">
tf
</th>
<th style="text-align:right;">
idf
</th>
<th style="text-align:right;">
tf-idf
</th>
<th style="text-align:right;">
rank by tf
</th>
<th style="text-align:right;">
rank by idf
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
data
</td>
<td style="text-align:right;">
21
</td>
<td style="text-align:right;">
540
</td>
<td style="text-align:right;">
0.039
</td>
<td style="text-align:right;">
0.000
</td>
<td style="text-align:right;">
0.000
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
266
</td>
</tr>
<tr>
<td style="text-align:left;">
tableau
</td>
<td style="text-align:right;">
11
</td>
<td style="text-align:right;">
540
</td>
<td style="text-align:right;">
0.020
</td>
<td style="text-align:right;">
0.875
</td>
<td style="text-align:right;">
0.018
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
1
</td>
</tr>
<tr>
<td style="text-align:left;">
work
</td>
<td style="text-align:right;">
10
</td>
<td style="text-align:right;">
540
</td>
<td style="text-align:right;">
0.019
</td>
<td style="text-align:right;">
0.000
</td>
<td style="text-align:right;">
0.000
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
271
</td>
</tr>
<tr>
<td style="text-align:left;">
book
</td>
<td style="text-align:right;">
9
</td>
<td style="text-align:right;">
540
</td>
<td style="text-align:right;">
0.017
</td>
<td style="text-align:right;">
0.087
</td>
<td style="text-align:right;">
0.001
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
246
</td>
</tr>
<tr>
<td style="text-align:left;">
good
</td>
<td style="text-align:right;">
8
</td>
<td style="text-align:right;">
540
</td>
<td style="text-align:right;">
0.015
</td>
<td style="text-align:right;">
0.087
</td>
<td style="text-align:right;">
0.001
</td>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
248
</td>
</tr>
<tr>
<td style="text-align:left;">
learn
</td>
<td style="text-align:right;">
8
</td>
<td style="text-align:right;">
540
</td>
<td style="text-align:right;">
0.015
</td>
<td style="text-align:right;">
0.000
</td>
<td style="text-align:right;">
0.000
</td>
<td style="text-align:right;">
6
</td>
<td style="text-align:right;">
268
</td>
</tr>
<tr>
<td style="text-align:left;">
amaze
</td>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
540
</td>
<td style="text-align:right;">
0.009
</td>
<td style="text-align:right;">
1.792
</td>
<td style="text-align:right;">
0.017
</td>
<td style="text-align:right;">
12
</td>
<td style="text-align:right;">
2
</td>
</tr>
<tr>
<td style="text-align:left;">
fun
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
540
</td>
<td style="text-align:right;">
0.007
</td>
<td style="text-align:right;">
1.386
</td>
<td style="text-align:right;">
0.010
</td>
<td style="text-align:right;">
24
</td>
<td style="text-align:right;">
6
</td>
</tr>
<tr>
<td style="text-align:left;">
single
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
540
</td>
<td style="text-align:right;">
0.007
</td>
<td style="text-align:right;">
1.792
</td>
<td style="text-align:right;">
0.013
</td>
<td style="text-align:right;">
28
</td>
<td style="text-align:right;">
4
</td>
</tr>
<tr>
<td style="text-align:left;">
week
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:right;">
540
</td>
<td style="text-align:right;">
0.007
</td>
<td style="text-align:right;">
1.792
</td>
<td style="text-align:right;">
0.013
</td>
<td style="text-align:right;">
33
</td>
<td style="text-align:right;">
5
</td>
</tr>
<tr>
<td style="text-align:left;">
copy
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
540
</td>
<td style="text-align:right;">
0.006
</td>
<td style="text-align:right;">
2.485
</td>
<td style="text-align:right;">
0.014
</td>
<td style="text-align:right;">
40
</td>
<td style="text-align:right;">
3
</td>
</tr>
</tbody>
</table>
<p><br></p>
<p><strong>References:</strong></p>
<ul>
<li>Learn about text analysis in a tidy way with <a href="https://www.tidytextmining.com/">Text Mining with R</a></li>
<li>R code for this blog post is available at my <a href="https://github.com/dmitrijsk/blogdown-dmitrijskass/blob/redo-swd-artisanal-data/content/post/2019-05-08-swdchallenge-artisanal-data.R">GitHub repository</a>.</li>
</ul>
<p><br></p>
<p>Any comments or suggestions? I’d be glad to know! Please leave them below, no login required if you check “I’d rather post as a guest”.</p>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>I heard the phrase “common locally and rare globally” for the first time <a href="https://www.coursera.org/lecture/ml-foundations/calculating-tf-idf-vectors-1rg5n">here</a> and I liked this definition a lot.<a href="#fnref1" class="footnote-back">↩</a></p></li>
<li id="fn2"><p>Stop words from the <a href="http://www.jmlr.org/papers/volume5/lewis04a/lewis04a.pdf">SMART information retrieval system</a> available in <code>tm</code> package served as the basis. They were supplemented with the names of podcast host and guests. One stop word was removed, it is a letter <em>“r”</em>, which is used by one of the podcast guests to refer to R as a software. It is definitely worth knowing prepackaged stop words!<a href="#fnref2" class="footnote-back">↩</a></p></li>
</ol>
</div>
