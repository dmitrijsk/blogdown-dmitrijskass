---
title: Effect of (not) standardizing variables in k-means clustering
author: Dmitrijs Kass
date: '2019-07-27'
slug: effect-of-not-standardizing-variables-in-k-means-clustering
categories:
  - clustering
tags: []
output:
  blogdown::html_page:
    toc: true
bibliography: effect-of-not-standardizing-variables-in-k-means-clustering.bib
---

```{r echo = FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, fig.align = 'center', fig.height=3.5, fig.width=7)
```


The process of clustering usually involves variable standardization. This post illustrates the effect of this data pre-processing step on the result of k-means clustering using a small data set of iPhones.

----

# Clustering, distances, k-means

Clustering refers to a very broad set of techniques for finding subgroups, or clusters, in a data set. When we cluster the observations of a data set, we seek to partition them into distinct groups so that 

* the observations within each group are quite "similar" to each other, 
* while observations in different groups are quite "different" from each other.

There are many possible ways to define the concept of "(dis)similarity", but by far the most common choice involves squared Euclidean distance [@james_witten_hastie_tibshirani_2017, pp. 385-387]. 

The Euclidean distance of two points $(x_1, y_1)$ and $(x_2,y_2)$ in a 2-dimensional space is calculated as

$$\sqrt{(x_2−x_1)^2+(y_2−y_1)^2}\text{.}$$
For example, if we are given two points with coordinates (1, 2) and (4, 6) then the Euclidean distance $d$ between these points is 5:

```{r plot_distance, fig.height=1.5, fig.width=5}
library(tidyverse)
tibble(x = c(1,4),
       y = c(2,6)) %>% 
  ggplot(aes(x, y)) +
  geom_point(size = 2) +
  geom_line() +
  coord_equal(ratio = 1, clip = "off", ylim = c(1.5, 7.5)) +
  annotate(geom = "text", x = -0.1, y = 2, label = "(1, 2)") +
  annotate(geom = "text", x = 5.1, y = 6, label = "(4, 6)") +
  annotate(geom = "text", x = 7, y = 3.9, label = expression(paste(italic(d) == sqrt((4-1)^2 + (6-2)^2)) == 5)) +
  theme_void()
```

Let's take k-means clustering. The algorithm aims to partition observations into *k* groups such that the sum of squared *distances* from observations to the assigned cluster centers is minimized. In the example plotted below, there are two distinct clusters ($k=2$) with 10 observations in each. Cluster centers are highlighted with coloured crosses.

```{r plot_clusters, fig.height=3, fig.width=4}
set.seed(1)
x <- tibble(x = c(rnorm(n = 10, mean = 0.5, sd = 0.4), rnorm(n = 10, mean = 2.5, sd = 0.4)),
       y = c(rnorm(n = 10, mean = 0.5, sd = 0.4), rnorm(n = 10, mean = 2.5, sd = 0.4)),
       cluster = rep(1:2, each = 10))
x_fit <- kmeans(x, centers = 2)
x_centers <- as_tibble(x_fit$centers) %>% mutate(cluster = 1:2)
x %>% 
  ggplot(aes(x, y, colour = as.factor(cluster))) +
  geom_point(size = 2, alpha = 0.4, show.legend = FALSE) +
  geom_point(size = 6, shape = "x", data = x_centers, mapping = aes(colour = as.factor(cluster))) +
  coord_equal(xlim = c(0, 3), ylim = c(0, 3)) +
  labs(subtitle = "Two hypothetical clusters",
       colour = "Cluster centers") +
  theme_classic()
```


# Surprise

Imagine you have observed prices and weights of four iPhone models. Here is the data (also available [here](/post/2019-07-27-effect-of-not-standardizing-variables-in-k-means-clustering_files/iphone_dataset.csv)): 

<!-- TODO: check link -->

```{r echo = TRUE}
# Attach packages.
library(tidyverse)
# Import phone data.
phones_df <- read_delim(file = "iphone_dataset.csv", delim = ";", skip = 2)
```


```{r}
# Print phone data.
phones_df %>% 
  knitr::kable(col.names = c("Brand", "Model", "Price (EUR)", "Weight (g)"), format = "html") %>% 
  kableExtra::kable_styling(full_width = FALSE, bootstrap_options = c("striped", "condensed"))
```

The plot below depicts four iPhone models in two dimensions: price in euro and weight in grams. Please do a mental exercise and assign these four iPhone models into two clusters.

```{r plot_surprise_1_question, fig.height=3, fig.width=6.5}
phones_df %>% 
  ggplot(aes(x = weight_g, y = price_eur)) + 
  geom_point(size = 4) +
  geom_text(aes(label = model), size = 3.5, vjust = "top", nudge_y = -40) +
  scale_x_continuous(expand = expand_scale(mult = 0.3)) +
  scale_y_continuous(expand = expand_scale(mult = 0.3)) +
  labs(title = "How would you assign these four iPhones models into two clusters?",
       x = "weight (g)", 
       y = "price (EUR)") +
  coord_equal(ratio = 1/20) +
  theme_classic()
```

Ready?

Did you assign iPhone 6 and 4 into one cluster and iPhone 8 Plus and 6s Plus into another? That's not what k-means would do without standardizing the variables.



# Original variables

Let's check by performing k-means with $k=2$. 

```{r echo = TRUE}
# Data for clustering.
phones_cl <- phones_df[c("price_eur", "weight_g")]
# Reproducibility.
set.seed(4)
# K-means with original variables.
kmeans_orig <- kmeans(phones_cl, centers = 2)
```

The result is plotted below with iPhone 8 Plus being alone in cluster 1 and all other phones in cluster 2. Hopefully, you are surprised.

```{r plot_kmeans_orig}
# Cluster centers.
centers_orig_df <- as_tibble(kmeans_orig$centers) %>%
  mutate(cluster = 1:2)

# K-means with original variables, unequal axes.
p <- phones_df %>% 
  mutate(cluster = kmeans_orig$cluster) %>% 
  ggplot(aes(x = weight_g, y = price_eur, color = as.factor(cluster))) + 
  geom_point(size = 3, show.legend = FALSE) +
  geom_point(data = centers_orig_df, size = 6, shape = "x") +
  scale_x_continuous(expand = expand_scale(mult = 0.3)) +
  scale_y_continuous(expand = expand_scale(mult = 0.3)) +
  labs(title = "Did you expect to see iPhone 8 Plus alone in cluster 1?",
       subtitle = "k-means clustering with original variables",
       x = "weight (g)", 
       y = "price (EUR)", 
       color = "Cluster centers") +
  coord_equal(ratio = 1/20) +
  theme_classic() +
  theme(legend.position = "top")

p +
  geom_text(aes(label = model), size = 3.5, hjust = "left", nudge_x = 2, show.legend = FALSE)
```

The catch is in the visualization - notice that x and y axes have different scales. One unit on the x axis (i.e. 1 gram) corresponds to exactly 20 units on the y axis (i.e. 20 euro). It means that the Euclidean distance between 125 and 150 grams (difference of 25 grams) on your screen exactly equals the distance between 250 and 750 euros (difference of 500 euros). As a result, diagonal distances are hard to evaluate visually. It seems that iPhone 6s Plus is significantly closer to iPhone 8 Plus than to iPhone 6. But it's an illusion. Note the distances on the plot below:

```{r plot_kmeans_orig_with_dist}
# distances <- as.matrix(dist(phones_cl))
# round(distances["iPhone 6s Plus", "iPhone 6"], 1)
# round(distances["iPhone 6s Plus", "iPhone 8 Plus"], 1)     

p +
  geom_text(aes(label = model), size = 3.5, hjust = "left", nudge_x = 3, nudge_y = -40, show.legend = FALSE) +
  labs(title = "Pay attention to the scales of axes") +
  # Add two ditances for annotation.
  # To the left.
  annotate(geom = "segment", x = 192, y = 470, xend = 129, yend = 360, 
           arrow = arrow(length = unit(x = 0.3, units = "cm"), ends = "last")) +
  annotate(geom = "text", x = 160.5, y = 415+65, label = expression(italic(d) == 126.8)) +
  # To the top.
  annotate(geom = "segment", x = 192, y = 470, xend = 202, yend = 770, 
           arrow = arrow(length = unit(x = 0.3, units = "cm"), ends = "last")) +
  annotate(geom = "text", x = 197-10, y = 620, label = expression(italic(d) == 300.2))


#                       price_eur  weight_g
# Apple	iPhone 6s Plus	470	              192	
# Apple	iPhone 6	      360	              129	
# Apple	iPhone 8 Plus	  770	              202	
```

All pairwise Euclidean distances can be calculated with `dist()`:

```{r echo = TRUE}
# Add rownames to see phone models in a distance matrix.
phones_cl <- as.data.frame(phones_cl)
rownames(phones_cl) <- phones_df$model
# Euclidean distances.
round(dist(phones_cl), 1)
```

<!-- Notice how far iPhone 8 Plus is from all other phones. -->

<!--  -->

If we re-plot the same data set while ensuring that one unit on the x-axis (i.e. 1 gram) has the same length as one unit on the y-axis (i.e. 1 euro), the k-means result is no more a surprise:

```{r plot_kmeans_orig_coord_equal}
# Adjust axes to the same scale.
p +
  geom_text(aes(label = model), size = 3.5, hjust = "left", nudge_x = 60, show.legend = FALSE) +
  labs(title = "No more surprise after equally scaling axes") +
  coord_equal(xlim = c(0, 800), ylim = c(0, 800), expand = FALSE)
```

Now this clustering may look correct. However, acknowledge that if iPhone weights were instead recorded in milligrams (i.e. multiply current weight values with 1,000) then prices would no longer affect the clustering results. The weight would become the only variable affecting the clustering. **In general, variables with higher variation have a higher impact on the Euclidean distance calculations and, as a result, clustering results.** We can resolve this problem by standardizing the data prior to the clustering.




# Standardized variables

Different standardization methods are available, including z-standardization (also called z-score standardization) and range standardization. Z-standardization rescales each variable by subtracting the mean $\bar{x}$ and dividing by standard deviation $s$:

$$Z=\frac{X-\bar{x}}{s}.$$

After z-standardization each variable has a mean $\bar{x}$ of 0 and a standard deviation $s$ of 1. Z-standardization can be done with `scale()`. 

In cluster analysis, however, range standardization (e.g., to a range of 0 to 1) typically works better [@milligan_cooper]. Range standardization requires subtracting the minimum value and then dividing it by the range (i.e., the difference between the maximum and minimum value):

$$R = \frac{X - X_{min}}{X_{max} - X_{min}}$$

We can write a one-line function to perform range standardization in R:

```{r echo = TRUE}
standardize_range <- function(x) {(x - min(x)) / (max(x) - min(x))}
```

Then apply our new function to each column with `apply()`:

```{r echo = TRUE}
phones_cl_scaled <- apply(phones_cl, MARGIN = 2, FUN = standardize_range)
phones_cl_scaled
```

Notice that each column now has a range of one.

Repeat k-means clustering with $k=2$ using standardized variables:

```{r echo = TRUE}
set.seed(1)
kmeans_scaled <- kmeans(phones_cl_scaled, centers = 2)
```

And here is the result:

```{r plot_kmeans_scaled_standardized_measurements}
var_min_vec <- apply(phones_cl, MARGIN = 2, FUN = min)
var_range_vec <- apply(phones_cl, MARGIN = 2, FUN = function(x) diff(range(x)))

centers_scaled_df <- tibble(price_eur_scaled = kmeans_scaled$centers[, 1],
       weight_g_scaled = kmeans_scaled$centers[, 2]) %>%
  bind_cols(as_tibble(t(apply(X = kmeans_scaled$centers, MARGIN = 1, FUN = function(x) x * var_range_vec + var_min_vec)))) %>%
  mutate(cluster = 1:2)

phones_df %>%
  mutate(price_eur_scaled = standardize_range(price_eur),
         weight_g_scaled         = standardize_range(weight_g),
         cluster = kmeans_scaled$cluster) %>%
  ggplot(aes(x = weight_g_scaled, y = price_eur_scaled, color = as.factor(cluster))) +
  geom_point(size = 3, show.legend = FALSE) +
  geom_point(data = centers_scaled_df, aes(x = weight_g_scaled, y = price_eur_scaled), size = 6, shape = "x") +
  labs(title = "Same scale axes, standardized units",
       subtitle = "k-means clustering with scaled variables",
       x = "standardized weight",
       y = "standardized price", color = "Cluster centers") +
  theme_classic() +
  theme(legend.position = "top") +
  coord_equal(xlim = c(0, 1), ylim = c(0, 1), clip = "off") +
  geom_text(aes(label = model), size = 3.5, hjust = "left", nudge_x = 0.07, show.legend = FALSE)
```

iPhone 4 and 6 are now in cluster 1 and iPhone 8 Plus and 6s Plus are in cluster 2. Below are distances calculated from standardized units:

```{r echo = TRUE}
round(dist(phones_cl_scaled), 1)
```


----

To summarize, below is the side-by-side illustration of the effect of variable standardization on k-means clustering: with original variables (left) and standardized variables (right).

```{r plot_kmeans_combined, fig.height=4, fig.width=7}
centers_combined <- bind_rows(
  centers_orig_df %>% mutate(facet = "Original variables"),
  centers_scaled_df %>% select(-contains("scaled")) %>% mutate(facet = "Standardized variables")
)

bind_rows(
  phones_df %>% 
    mutate(cluster = kmeans_orig$cluster,
           facet = "Original variables"),
  phones_df %>% 
    mutate(cluster = kmeans_scaled$cluster,
           facet = "Standardized variables")
) %>%

  ggplot(aes(x = weight_g, y = price_eur, color = as.factor(cluster))) +
  facet_grid(~facet) +
  coord_equal(xlim = c(0, 800), ylim = c(0, 800)) +
  geom_point(size = 3, show.legend = FALSE) +
  geom_text(aes(label = model), size = 3.5, hjust = "left", nudge_x = 60, show.legend = FALSE) +
  geom_point(data = centers_combined, aes(x = weight_g, y = price_eur), size = 6, shape = "x") +
  labs(title = "Effect of (not) standardizing variables in k-means clustering",
       x = "weight (g)",
       y = "price (EUR)",
       color = "Cluster centers") +
  theme_classic() +
  theme(legend.position = "top", 
        strip.background = element_rect(colour = "white"), 
        strip.text = element_text(face = "bold", size = 11), legend.title = element_text(size = 10), 
        panel.spacing = unit(1.5, units = "cm"), plot.title = element_text(hjust = 0.5))
```




# Conclusions

Beware of the differences in variation of variables used for clustering as variables with higher variation have a higher impact on clustering. The choice of variable standardization methods should depend on the type of data and problem at hand and there is no single right method of standardization. [@james_witten_hastie_tibshirani_2017, pp. 400] suggest trying several different choices and looking for the one with the most useful or interpretable solution. [@milligan_cooper] show that the range standardization typically works better for clustering.

<!-- TODO: Add about outlier - that z-score is not good when they are present. And maybe use median/mad instead. -->

<hr>

I would appreciate any comments or suggestions. Please leave them below, no login required if you check "I'd rather post as a guest".

# References
