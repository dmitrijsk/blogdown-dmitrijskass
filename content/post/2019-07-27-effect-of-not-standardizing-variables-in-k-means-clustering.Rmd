---
title: Effect of (not) standardizing variables in k-means clustering
author: Dmitrijs Kass
date: '2019-07-25'
slug: effect-of-not-standardizing-variables-in-k-means-clustering
categories:
  - clustering
tags: []
output:
  blogdown::html_page:
    toc: true
bibliography: bibliography.bib
---

```{r echo = FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, fig.align = 'center', fig.height=3.5, fig.width=7)
```


The process of clustering usually involves variable scaling / standardization. This post illustrates the effect of this data pre-processing step on the result of k-means clustering using a small data set of iPhones.

----

# Clustering, distances, k-means

Clustering refers to a very broad set of techniques for finding subgroups, or clusters, in a data set. When we cluster the observations of a data set, we seek to partition them into distinct groups so that 

* the observations within each group are quite "similar" to each other, 
* while observations in different groups are quite "different" from each other.

There are many possible ways to define the concept of "(dis)similarity", but by far the most common choice involves squared Euclidean distance [@james_witten_hastie_tibshirani_2017, pp. 385]. 

The Euclidean distance of two points $(x_1, y_1)$ and $(x_2,y_2)$ in a 2-dimensional space is calculated as

$$\sqrt{(x_2−x_1)^2+(y_2−y_1)^2}\text{.}$$
For example, if we are given two points with coordinates (1, 2) and (4, 6) then the Euclidean distance $d$ between these points is 5:


```{r plot_distance, fig.height=1.5, fig.width=5}
library(tidyverse)
tibble(x = c(1,4),
       y = c(2,6)) %>% 
  ggplot(aes(x, y)) +
  geom_point(size = 2) +
  geom_line() +
  coord_equal(ratio = 1, clip = "off", ylim = c(1.5, 7.5)) +
  annotate(geom = "text", x = -0.1, y = 2, label = "(1, 2)") +
  annotate(geom = "text", x = 5.1, y = 6, label = "(4, 6)") +
  annotate(geom = "text", x = 7, y = 3.9, label = expression(paste(italic(d) == sqrt((4-1)^2 + (6-2)^2)) == 5)) +
  theme_void()
```

Let's take k-means clustering. The algorithm aims to partition observations into *k* groups such that the sum of squared *distances* from observations to the assigned cluster centers is minimized. In the example plotted below, there are two distinct clusters ($k=2$) with 10 observations in each. Cluster centers are highlighted with coloured crosses.

```{r plot_clusters, fig.height=3, fig.width=4}
set.seed(1)
x <- tibble(x = c(rnorm(n = 10, mean = 0.5, sd = 0.4), rnorm(n = 10, mean = 2.5, sd = 0.4)),
       y = c(rnorm(n = 10, mean = 0.5, sd = 0.4), rnorm(n = 10, mean = 2.5, sd = 0.4)))
x_fit <- kmeans(x, centers = 2)
x_centers <- as_tibble(x_fit$centers) %>% mutate(cluster = 1:2)
x %>% 
  ggplot(aes(x, y)) +
  geom_point(size = 2) +
  geom_point(size = 6, shape = "x", data = x_centers, mapping = aes(colour = as.factor(cluster))) +
  coord_equal(xlim = c(0, 3), ylim = c(0, 3)) +
  labs(subtitle = "Two hypothetical clusters",
       colour = "Cluster centers") +
  theme_classic()
```


# Surprise

Imagine you have observed prices and weights of four iPhone models. Here is the data (also available [here](iphone_dataset.csv)): 

```{r echo = TRUE}
# Attach packages.
library(tidyverse)
# Import phone data.
phones_df <- read_delim(file = "iphone_dataset.csv", delim = ";", skip = 2)
```


```{r}
# Print phone data.
phones_df %>% 
  knitr::kable(col.names = c("Brand", "Model", "Approx. price (EUR)", "Weight (g)"), format = "html") %>% 
  kableExtra::kable_styling(full_width = FALSE, bootstrap_options = c("striped", "condensed"))
```

The plot below depicts four iPhone models in two dimensions: approximate price in euro and weight in grams. Please do a mental exercise and assign these four iPhone models into two clusters. Which phones did you assign into cluster 1 and which into cluster 2?

```{r plot_surprise_1_question, fig.height=3, fig.width=6.5}
phones_df %>% 
  ggplot(aes(x = weight_g, y = approx_price_eur)) + 
  geom_point(size = 4) +
  geom_text(aes(label = model), size = 3.5, vjust = "top", nudge_y = -40) +
  scale_x_continuous(expand = expand_scale(mult = 0.3)) +
  scale_y_continuous(expand = expand_scale(mult = 0.3)) +
  labs(title = "How would you assign these four iPhones models into two clusters?",
       x = "weight (g)", 
       y = "price (EUR)") +
  coord_equal(ratio = 1/20) +
  theme_classic()
```

Did you assign iPhone 6 and 4 into one cluster and iPhone 8 Plus and 6s Plus into another? That's not what k-means would do without standardizing the variables.



# Original variables

We are doing k-means with $k=2$. 

```{r echo = TRUE}
# Data for clustering.
phones_cl <- phones_df[c("approx_price_eur", "weight_g")]
# Reproducibility.
set.seed(4)
# K-means with original variables.
kmeans_orig <- kmeans(phones_cl, centers = 2)
```

The result is plotted below with iPhone 8 Plus being alone in cluster 1 and all other phones in cluster 2. Hopefully, you are surprised.

```{r plot_kmeans_orig}
# Cluster centers.
centers_orig_df <- as_tibble(kmeans_orig$centers) %>%
  mutate(cluster = 1:2)

# K-means with original variables, unequal axes.
p <- phones_df %>% 
  mutate(cluster = kmeans_orig$cluster) %>% 
  ggplot(aes(x = weight_g, y = approx_price_eur, color = as.factor(cluster))) + 
  geom_point(size = 3, show.legend = FALSE) +
  geom_point(data = centers_orig_df, size = 6, shape = "x") +
  scale_x_continuous(expand = expand_scale(mult = 0.3)) +
  scale_y_continuous(expand = expand_scale(mult = 0.3)) +
  labs(title = "Did you expect to see iPhone 8 Plus alone in cluster 1?",
       subtitle = "k-means clustering with original variables",
       x = "weight (g)", 
       y = "price (EUR)", 
       color = "Cluster centers") +
  coord_equal(ratio = 1/20) +
  theme_classic() +
  theme(legend.position = "top")

p +
  geom_text(aes(label = model), size = 3.5, hjust = "left", nudge_x = 2, show.legend = FALSE)
```

The catch is in the visualization - notice that x and y axes have different scales. Actually, one unit on the y axis (i.e. 1 euro) corresponds to exactly 20 units on the x axis (i.e. 20 grams). It means that the Euclidean distance between 125 and 150 grams (difference of 25 grams) on your screen exactly equals the distance between 250 and 750 euros (difference of 500 euros). As a result, diagonal distances are hard to evaluate visually. It seems that iPhone 6s Plus is significantly closer to iPhone 8 Plus than to iPhone 6. But is only seems, as shown on the plot below:

```{r plot_kmeans_orig_with_dist}
p +
  geom_text(aes(label = model), size = 3.5, hjust = "left", nudge_x = 2, show.legend = FALSE) +
  labs(title = "Pay attential to the scales of axes") +
  # Add two ditances for annotation.
  annotate(geom = "segment", x = 192, y = 470, xend = 129, yend = 360)



#                       approx_price_eur  weight_g
# Apple	iPhone 6s Plus	470	              192	
# Apple	iPhone 6	      360	              129	
# Apple	iPhone 8 Plus	  770	              202	
```




```{r}
price_range_vec <- range(phones_df$approx_price_eur)
min_price <- price_range_vec[1]
max_price <- price_range_vec[2]
price_range <- diff(price_range_vec)

weight_range_vec <- range(phones_df$weight_g)
min_weight <- weight_range_vec[1]
max_weight <- weight_range_vec[2]
weight_range <- diff(weight_range_vec)

range_ratio <- round(price_range / weight_range, 1)
```

The trick is in the variability of prices and weights. The range of prices is `r min_price` - `r max_price` = `r price_range`, while the range of weights is `r min_weight` - `r max_weight` = `r weight_range`. The price range is `r range_ratio` times greater than the weight range. It means that in a calculation of Euclidean distances, 1 euro difference in price is equivalent to `r range_ratio` gram difference in weight. Thus, a small difference in price may outweigh a big difference in weight. In other words, price affects k-means results more than weight.

If we re-plot the same data set while ensuring that one unit on the x-axis (euros) has the same length as one unit on the y-axis (grams), the k-means result is no more a surprise:

```{r plot_kmeans_orig_coord_equal}
# Adjust axes to the same scale.
p +
  geom_text(aes(label = model), size = 3.5, hjust = "left", nudge_x = 60, show.legend = FALSE) +
  labs(title = "No more surprise after equally scaling axes") +
  coord_equal(xlim = c(0, 800), ylim = c(0, 800), expand = FALSE)
```

Here are pairwise Euclidean distances calculated with `dist()`:

```{r echo = TRUE}
# Add rownames to see phone models in a distance matrix.
phones_cl <- as.data.frame(phones_cl)
rownames(phones_cl) <- phones_df$model
# Euclidean distances.
round(dist(phones_cl), 1)
```

Notice how far iPhone 8 Plus is from all other phones.


# Standardized variables

Consider standardizing each variable by subtracting the mean $\bar{x}$ (i.e. centering) and dividing by its standard deviation $s$:

$$Z=\frac{X-\bar{x}}{s}.$$

This equalizes the variability in data ($s=1$ for each variable) and makes price and weight equally important for k-means clustering. Here are variables standardized with `scale()`:

```{r echo = TRUE}
phones_cl_scaled <- scale(phones_cl)
as_tibble(phones_cl_scaled)
```

And corresponding distances:

```{r echo = TRUE}
round(dist(phones_cl_scaled), 1)
```

Notice that iPhone 6s Plus is now closest to iPhone 8 Plus. K-means clustering changes accordingly.

```{r echo = TRUE}
kmeans_scaled <- kmeans(phones_cl_scaled, centers = 2)
```

The plot below shows k-means clustering using standardized variables:

```{r plot_kmeans_scaled_standardized_measurements}
feature_centers_vec <- attributes(phones_cl_scaled)[["scaled:center"]]
feature_sd_vec      <- attributes(phones_cl_scaled)[["scaled:scale"]]

centers_scaled_df <- tibble(approx_price_eur_scaled = kmeans_scaled$centers[, 1],
       weight_g_scaled = kmeans_scaled$centers[, 2]) %>%
  bind_cols(as_tibble(t(apply(X = kmeans_scaled$centers, MARGIN = 1, FUN = function(x) x * feature_sd_vec + feature_centers_vec)))) %>%
  mutate(cluster = 1:2)

phones_df %>%
  mutate(approx_price_eur_scaled = (approx_price_eur - mean(approx_price_eur)) / sd(approx_price_eur),
         weight_g_scaled = (weight_g - mean(weight_g)) / sd(weight_g),
         cluster = kmeans_scaled$cluster) %>%
  ggplot(aes(x = weight_g_scaled, y = approx_price_eur_scaled, color = as.factor(cluster))) +
  geom_point(size = 3, show.legend = FALSE) +
  geom_point(data = centers_scaled_df, aes(x = weight_g_scaled, y = approx_price_eur_scaled), size = 6, shape = "x") +
  labs(title = "Same scale axes, standardized units",
       subtitle = "k-means clustering with scaled variables",
       x = "standardized weight",
       y = "standardized price", color = "Cluster centers") +
  theme_classic() +
  theme(legend.position = "top") +
  coord_equal(xlim = c(-2, 2), ylim = c(-2, 2), clip = "off") +
  geom_text(aes(label = model), size = 3.5, hjust = "left", nudge_x = 0.3, show.legend = FALSE)
```

----

To summarize, here are side-by-side k-means clustering results for $k=2$ with original variables (left) and standardized variables (right):

```{r plot_kmeans_combined, fig.height=4, fig.width=7}
centers_combined <- bind_rows(
  centers_orig_df %>% mutate(facet = "Original variables"),
  centers_scaled_df %>% select(-contains("scaled")) %>% mutate(facet = "Standardized variables")
)

bind_rows(
  phones_df %>% 
    mutate(cluster = kmeans_orig$cluster,
           facet = "Original variables"),
  phones_df %>% 
    mutate(cluster = kmeans_scaled$cluster,
           facet = "Standardized variables")
) %>%

  ggplot(aes(x = weight_g, y = approx_price_eur, color = as.factor(cluster))) +
  facet_grid(~facet) +
  coord_equal(xlim = c(0, 800), ylim = c(0, 800)) +
  geom_point(size = 3, show.legend = FALSE) +
  geom_text(aes(label = model), size = 3.5, hjust = "left", nudge_x = 60, show.legend = FALSE) +
  geom_point(data = centers_combined, aes(x = weight_g, y = approx_price_eur), size = 6, shape = "x") +
  labs(title = "Effect of (not) standardizing variables in k-means clustering",
       x = "weight (g)",
       y = "price (EUR)",
       color = "Cluster centers") +
  theme_classic() +
  theme(legend.position = "top", 
        strip.background = element_rect(colour = "white"), 
        strip.text = element_text(face = "bold", size = 11), legend.title = element_text(size = 10), 
        panel.spacing = unit(1.5, units = "cm"), plot.title = element_text(hjust = 0.5))
```

<hr>

I would appreciate any comments or suggestions. Please leave them below, no login required if you check "I'd rather post as a guest".

# References