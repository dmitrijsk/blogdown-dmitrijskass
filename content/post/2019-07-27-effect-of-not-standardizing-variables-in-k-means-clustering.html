---
title: Effect of (not) standardizing variables in k-means clustering
author: Dmitrijs Kass
date: '2019-07-27'
slug: effect-of-not-standardizing-variables-in-k-means-clustering
categories:
  - clustering
tags: []
output:
  blogdown::html_page:
    toc: true
bibliography: effect-of-not-standardizing-variables-in-k-means-clustering.bib
---

<script src="/rmarkdown-libs/kePrint/kePrint.js"></script>

<div id="TOC">
<ul>
<li><a href="#clustering-distances-k-means">Clustering, distances, k-means</a></li>
<li><a href="#surprise">Surprise</a></li>
<li><a href="#original-variables">Original variables</a></li>
<li><a href="#standardized-variables">Standardized variables</a></li>
<li><a href="#results">Results</a></li>
<li><a href="#conclusions">Conclusions</a></li>
<li><a href="#references">References</a></li>
</ul>
</div>

<p>The process of clustering usually involves variable standardization. This post illustrates the effect of this important data pre-processing step on the result of k-means clustering using a small data set of iPhones, with <a href="https://www.r-project.org/">R</a>.</p>
<hr />
<div id="clustering-distances-k-means" class="section level1">
<h1>Clustering, distances, k-means</h1>
<p>Clustering refers to a very broad set of techniques for finding subgroups, or clusters, in a data set. When we cluster the observations of a data set, we seek to partition them into distinct groups so that</p>
<ul>
<li>the observations within each group are quite “similar” to each other,</li>
<li>while observations in different groups are quite “different” from each other.</li>
</ul>
<p>There are many possible ways to define the concept of “(dis)similarity”, but by far the most common choice involves squared Euclidean distance <span class="citation">(James et al. 2017, 385–87)</span>.</p>
<p>The Euclidean distance of two points <span class="math inline">\((x_1, y_1)\)</span> and <span class="math inline">\((x_2,y_2)\)</span> in a 2-dimensional space is calculated as</p>
<p><span class="math display">\[\sqrt{(x_2−x_1)^2+(y_2−y_1)^2}\text{.}\]</span>
For example, if we are given two points with coordinates (1, 2) and (4, 6) then the Euclidean distance <span class="math inline">\(d\)</span> between these points is 5:</p>
<p><img src="/post/2019-07-27-effect-of-not-standardizing-variables-in-k-means-clustering_files/figure-html/plot_distance-1.png" width="480" style="display: block; margin: auto;" /></p>
<p>Let’s take k-means clustering. The algorithm aims to partition observations into <em>k</em> groups such that the sum of squared <em>distances</em> from observations to the assigned cluster centers is minimized. In the example plotted below, there are two distinct clusters (<span class="math inline">\(k=2\)</span>) with 10 observations in each. Cluster centers are highlighted with colored crosses.</p>
<p><img src="/post/2019-07-27-effect-of-not-standardizing-variables-in-k-means-clustering_files/figure-html/plot_clusters-1.png" width="384" style="display: block; margin: auto;" /></p>
</div>
<div id="surprise" class="section level1">
<h1>Surprise</h1>
<p>Imagine you have observed prices and weights of four iPhone models (available <a href="/post/2019-07-27-effect-of-not-standardizing-variables-in-k-means-clustering_files/iphone_dataset.csv">here</a>):</p>
<pre class="r"><code># Attach packages.
library(tidyverse)
# Import phone data.
phones_df &lt;- read_delim(file = &quot;iphone_dataset.csv&quot;, delim = &quot;;&quot;, skip = 2)</code></pre>
<table class="table table-condensed" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
Brand
</th>
<th style="text-align:left;">
Model
</th>
<th style="text-align:right;">
Price (EUR)
</th>
<th style="text-align:right;">
Weight (g)
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Apple
</td>
<td style="text-align:left;">
iPhone 4
</td>
<td style="text-align:right;">
200
</td>
<td style="text-align:right;">
137
</td>
</tr>
<tr>
<td style="text-align:left;">
Apple
</td>
<td style="text-align:left;">
iPhone 6
</td>
<td style="text-align:right;">
360
</td>
<td style="text-align:right;">
129
</td>
</tr>
<tr>
<td style="text-align:left;">
Apple
</td>
<td style="text-align:left;">
iPhone 6s Plus
</td>
<td style="text-align:right;">
470
</td>
<td style="text-align:right;">
192
</td>
</tr>
<tr>
<td style="text-align:left;">
Apple
</td>
<td style="text-align:left;">
iPhone 8 Plus
</td>
<td style="text-align:right;">
770
</td>
<td style="text-align:right;">
202
</td>
</tr>
</tbody>
</table>
<p>The plot below depicts four iPhone models in two dimensions: price in euro and weight in grams. Please do a mental exercise and assign these four iPhone models into two clusters.</p>
<p><img src="/post/2019-07-27-effect-of-not-standardizing-variables-in-k-means-clustering_files/figure-html/plot_surprise_1_question-1.png" width="624" style="display: block; margin: auto;" /></p>
<p>Ready?</p>
<p>Did you assign iPhone 6 and 4 into one cluster and iPhone 8 Plus and 6s Plus into another? That’s not what k-means would do without standardizing the variables.</p>
</div>
<div id="original-variables" class="section level1">
<h1>Original variables</h1>
<p>Let’s check by performing k-means with <span class="math inline">\(k=2\)</span>.</p>
<pre class="r"><code># Data for clustering.
phones_cl &lt;- phones_df[c(&quot;price_eur&quot;, &quot;weight_g&quot;)]
# Reproducibility.
set.seed(4)
# K-means with original variables.
kmeans_orig &lt;- kmeans(phones_cl, centers = 2)</code></pre>
<p>The result is plotted below with iPhone 8 Plus being alone in cluster 1 and all other phones in cluster 2. Hopefully, you are surprised.</p>
<p><img src="/post/2019-07-27-effect-of-not-standardizing-variables-in-k-means-clustering_files/figure-html/plot_kmeans_orig-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>The catch is in the visualization - notice that x and y axes have different scales. One unit on the x axis (i.e. 1 gram) corresponds to exactly 20 units on the y axis (i.e. 20 euro). It means that the Euclidean distance between 125 and 150 grams (difference of 25 grams) on your screen exactly equals the distance between 250 and 750 euros (difference of 500 euros). As a result, diagonal distances are hard to evaluate visually. It seems that iPhone 6s Plus is significantly closer to iPhone 8 Plus than to iPhone 6. But it’s an illusion. Note the distances on the plot below:</p>
<p><img src="/post/2019-07-27-effect-of-not-standardizing-variables-in-k-means-clustering_files/figure-html/plot_kmeans_orig_with_dist-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>All pairwise Euclidean distances can be calculated with <code>dist()</code>:</p>
<pre class="r"><code># Add rownames to see phone models in a distance matrix.
phones_cl &lt;- as.data.frame(phones_cl)
rownames(phones_cl) &lt;- phones_df$model
# Euclidean distances.
round(dist(phones_cl), 1)</code></pre>
<pre><code>##                iPhone 4 iPhone 6 iPhone 6s Plus
## iPhone 6          160.2                        
## iPhone 6s Plus    275.5    126.8               
## iPhone 8 Plus     573.7    416.4          300.2</code></pre>
<p>If we re-plot the same data set while ensuring that one unit on the x-axis (i.e. 1 gram) has the same length as one unit on the y-axis (i.e. 1 euro), the k-means result is no longer a surprise:</p>
<p><img src="/post/2019-07-27-effect-of-not-standardizing-variables-in-k-means-clustering_files/figure-html/plot_kmeans_orig_coord_equal-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Now this clustering may look correct. However, acknowledge that if iPhone weights were instead recorded in milligrams (i.e. multiply current weight values with 1,000) then prices would no longer affect the clustering results. The weight would become the only variable affecting the clustering. <strong>In general, variables with higher variation have a higher impact on the Euclidean distance calculations and, as a result, clustering results.</strong></p>
<p>So the results of clustering depend on the scales of measurements in the data set. We can resolve this problem by standardizing the data prior to the clustering.</p>
</div>
<div id="standardized-variables" class="section level1">
<h1>Standardized variables</h1>
<p>Different standardization methods are available, including z-standardization (also called z-score standardization) and range standardization. Z-standardization rescales each variable <span class="math inline">\(X\)</span> by subtracting its mean <span class="math inline">\(\bar{x}\)</span> and dividing by its standard deviation <span class="math inline">\(s\)</span>:</p>
<p><span class="math display">\[Z=\frac{X-\bar{x}}{s}.\]</span></p>
<p>After z-standardization each variable has a mean <span class="math inline">\(\bar{x}\)</span> of 0 and a standard deviation <span class="math inline">\(s\)</span> of 1. Z-standardization can be done with <code>scale()</code>.</p>
<p>In cluster analysis, however, range standardization (e.g., to a range of 0 to 1) typically works better <span class="citation">(Milligan and Cooper 1988)</span>. Range standardization requires subtracting the minimum value and then dividing it by the range (i.e., the difference between the maximum and minimum value):</p>
<p><span class="math display">\[R = \frac{X - X_{min}}{X_{max} - X_{min}}\]</span></p>
<p>We can write a one-line function to perform range standardization:</p>
<pre class="r"><code>standardize_range &lt;- function(x) {(x - min(x)) / (max(x) - min(x))}</code></pre>
<p>Then apply our new function to each column:</p>
<pre class="r"><code>phones_cl_scaled &lt;- apply(phones_cl, MARGIN = 2, FUN = standardize_range)
phones_cl_scaled</code></pre>
<pre><code>##                price_eur  weight_g
## iPhone 4       0.0000000 0.1095890
## iPhone 6       0.2807018 0.0000000
## iPhone 6s Plus 0.4736842 0.8630137
## iPhone 8 Plus  1.0000000 1.0000000</code></pre>
<p>Notice that each column now has a range of one.</p>
<p>Repeat k-means clustering using <em>standardized</em> variables:</p>
<pre class="r"><code>set.seed(1)
kmeans_scaled &lt;- kmeans(phones_cl_scaled, centers = 2)</code></pre>
<p>The results have changed: iPhone 4 and 6 are now in cluster 1 and iPhone 8 Plus and 6s Plus are in cluster 2.</p>
<p><img src="/post/2019-07-27-effect-of-not-standardizing-variables-in-k-means-clustering_files/figure-html/plot_kmeans_scaled_standardized_measurements-1.png" width="672" style="display: block; margin: auto;" /></p>
<!-- If you are interested, below are distances calculated from *standardized* units: -->
<!-- ```{r echo = TRUE} -->
<!-- round(dist(phones_cl_scaled), 1) -->
<!-- ``` -->
</div>
<div id="results" class="section level1">
<h1>Results</h1>
<p>Below is the side-by-side illustration of the effect of variable standardization on k-means clustering. Clustering plotted on the left uses original variables; clustering on the right - standardized variables.</p>
<p><img src="/post/2019-07-27-effect-of-not-standardizing-variables-in-k-means-clustering_files/figure-html/plot_kmeans_combined-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<div id="conclusions" class="section level1">
<h1>Conclusions</h1>
<p>Perform exploratory data analysis prior to clustering and beware of the differences in variation of variables. Variables with higher variation tend to have a higher impact on clustering. There is no single right method of standardization. <span class="citation">(James et al. 2017, 400)</span> suggest trying several different choices and looking for the one with the most useful or interpretable solution. <span class="citation">(Milligan and Cooper 1988)</span> show that the range standardization typically works better for hierarchical clustering and that z-standardization may even be significantly worse than no standardization in the presence of outliers. In case of outliers a possible alternative is to use the median absolute deviation instead of the standard deviation <span class="citation">(Spector 2011, 160)</span>.</p>
<hr>
<p>I would appreciate any comments or suggestions. Please leave them below, no login required if you check “I’d rather post as a guest”.</p>
</div>
<div id="references" class="section level1 unnumbered">
<h1>References</h1>
<div id="refs" class="references">
<div id="ref-james_witten_hastie_tibshirani_2017">
<p>James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2017. <em>An Introduction to Statistical Learning: With Applications in R</em>. Springer. <a href="http://faculty.marshall.usc.edu/gareth-james/ISL/">http://faculty.marshall.usc.edu/gareth-james/ISL/</a>.</p>
</div>
<div id="ref-milligan_cooper">
<p>Milligan, Glenn W., and Martha C. Cooper. 1988. “A Study of Standardization of Variables in Cluster Analysis.” <em>SpringerLink</em>. Springer-Verlag. <a href="https://doi.org/10.1007/BF01897163">https://doi.org/10.1007/BF01897163</a>.</p>
</div>
<div id="ref-stat_133">
<p>Spector, Phil. 2011. “Stat 133 Class Notes - Spring, 2011.” <a href="https://www.stat.berkeley.edu/~s133/all2011.pdf">https://www.stat.berkeley.edu/~s133/all2011.pdf</a>.</p>
</div>
</div>
</div>
