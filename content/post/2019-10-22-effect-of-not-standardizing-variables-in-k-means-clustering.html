---
title: Effect of (not) standardizing variables in k-means clustering
author: Dmitrijs Kass
date: '2019-10-22'
slug: effect-of-not-standardizing-variables-in-k-means-clustering
categories:
  - clustering
tags: []
output:
  blogdown::html_page:
    toc: true
bibliography: effect-of-not-standardizing-variables-in-k-means-clustering.bib
---

<script src="/rmarkdown-libs/kePrint/kePrint.js"></script>

<div id="TOC">
<ul>
<li><a href="#clustering-distances-k-means">Clustering, distances, k-means</a></li>
<li><a href="#surprise">Surprise</a></li>
<li><a href="#original-variables">Original variables</a></li>
<li><a href="#standardized-variables">Standardized variables</a></li>
<li><a href="#results">Results</a></li>
<li><a href="#conclusions">Conclusions</a></li>
<li><a href="#references">References</a></li>
</ul>
</div>

<p>The process of clustering usually involves variable standardization. This post illustrates the effect of this important data pre-processing step on the result of k-means clustering with <a href="https://www.r-project.org/">R</a> using a small data set of groceries shopping.</p>
<hr />
<div id="clustering-distances-k-means" class="section level1">
<h1>Clustering, distances, k-means</h1>
<p>Clustering refers to a very broad set of techniques for finding subgroups, or clusters, in a data set. When we cluster the observations of a data set, we seek to partition them into distinct groups so that</p>
<ul>
<li>the observations within each group are quite “similar” to each other,</li>
<li>while observations in different groups are quite “different” from each other.</li>
</ul>
<p>There are many possible ways to define the concept of “(dis)similarity”, but by far the most common choice involves squared Euclidean distance <span class="citation">(James et al. 2017, 385–87)</span>.</p>
<p>The Euclidean distance of two points <span class="math inline">\((x_1, y_1)\)</span> and <span class="math inline">\((x_2,y_2)\)</span> in a 2-dimensional space is calculated as</p>
<p><span class="math display">\[\sqrt{(x_2−x_1)^2+(y_2−y_1)^2}\text{.}\]</span>
For example, if we are given two points with coordinates (1, 2) and (4, 6) then the Euclidean distance <span class="math inline">\(d\)</span> between these points is 5:</p>
<p><img src="/post/2019-10-22-effect-of-not-standardizing-variables-in-k-means-clustering_files/figure-html/plot_distance-1.png" width="480" style="display: block; margin: auto;" /></p>
<p>Let’s take k-means clustering. The algorithm aims to partition observations into <em>k</em> groups such that the sum of squared <em>distances</em> from observations to the assigned cluster centers is minimized. In the example plotted below, there are two distinct clusters (<span class="math inline">\(k=2\)</span>) with 10 observations in each. Cluster centers are highlighted with colored crosses.</p>
<p><img src="/post/2019-10-22-effect-of-not-standardizing-variables-in-k-means-clustering_files/figure-html/plot_clusters-1.png" width="384" style="display: block; margin: auto;" /></p>
</div>
<div id="surprise" class="section level1">
<h1>Surprise</h1>
<p>Imagine you have a purchasing history of customers in a local shop. You would like to cluster them into groups with similar purchasing habits to target them later with different offers and marketing materials. For simplicity, you pick only four customers and their history of purchasing caviar and bread during the last month, both measured in kilograms. You aggregate data on a customer level and get the table below (available in csv <a href="/post/2019-07-27-effect-of-not-standardizing-variables-in-k-means-clustering_files/groceries.csv">here</a>):</p>
<pre class="r"><code># Attach packages.
library(tidyverse)
# Import groceries data.
groceries_df &lt;- read_delim(file = &quot;groceries.csv&quot;, delim = &quot;;&quot;)</code></pre>
<table class="table table-condensed" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
Customer
</th>
<th style="text-align:right;">
Caviar (kg)
</th>
<th style="text-align:right;">
Bread (kg)
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Artis
</td>
<td style="text-align:right;">
0.0
</td>
<td style="text-align:right;">
10.0
</td>
</tr>
<tr>
<td style="text-align:left;">
Baiba
</td>
<td style="text-align:right;">
0.1
</td>
<td style="text-align:right;">
9.0
</td>
</tr>
<tr>
<td style="text-align:left;">
Cintija
</td>
<td style="text-align:right;">
1.0
</td>
<td style="text-align:right;">
7.5
</td>
</tr>
<tr>
<td style="text-align:left;">
Dainis
</td>
<td style="text-align:right;">
1.1
</td>
<td style="text-align:right;">
4.5
</td>
</tr>
</tbody>
</table>
<p>Below is the same data in the scatter plot. Please do a mental exercise and assign these four customers into two clusters.</p>
<p><img src="/post/2019-10-22-effect-of-not-standardizing-variables-in-k-means-clustering_files/figure-html/plot_surprise_1_question-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>Ready?</p>
<p>Did you assign Artis with Baiba into one cluster and Cintija with Dainis into another? That’s not what k-means would do without standardizing the variables.</p>
</div>
<div id="original-variables" class="section level1">
<h1>Original variables</h1>
<p>Let’s check by performing k-means with <span class="math inline">\(k=2\)</span>.</p>
<pre class="r"><code># Data for clustering.
groceries_num &lt;- groceries_df[-1]
# Reproducibility.
set.seed(1)
# K-means with original variables.
kmeans_orig &lt;- kmeans(groceries_num, centers = 2)</code></pre>
<p>The result is plotted below with Dainis being alone in cluster 2 and all other customers in cluster 1. Hopefully, you are surprised.</p>
<p><img src="/post/2019-10-22-effect-of-not-standardizing-variables-in-k-means-clustering_files/figure-html/plot_kmeans_orig-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>The catch is in the visualization - notice that x and y axes have different scales. The length of one unit on the x axis (i.e. 1 kilo of caviar) is exactly 20 times longer than the length of one unit on the y axis (i.e. 1 kilo of bread). As a result, diagonal distances are hard to evaluate visually. It seems that Cintija is closer to Dainis than to Baiba. But it’s an illusion. Note the distances on the plot below:</p>
<p><img src="/post/2019-10-22-effect-of-not-standardizing-variables-in-k-means-clustering_files/figure-html/plot_kmeans_orig_with_dist-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>All pairwise Euclidean distances can be calculated with <code>dist()</code>:</p>
<pre class="r"><code># Add rownames to see customer names in a distance matrix.
groceries_num &lt;- as.data.frame(groceries_num)
rownames(groceries_num) &lt;- groceries_df$customer
# Euclidean distances in kilos.
round(dist(groceries_num), 1)</code></pre>
<pre><code>##         Artis Baiba Cintija
## Baiba     1.0              
## Cintija   2.7   1.7        
## Dainis    5.6   4.6     3.0</code></pre>
<p>If we re-plot the same data set while ensuring that one unit on the x-axis (i.e. 1 kilo of caviar) has the same length as one unit on the y-axis (i.e. 1 kilo of bread), the k-means result is no longer a surprise:</p>
<p><img src="/post/2019-10-22-effect-of-not-standardizing-variables-in-k-means-clustering_files/figure-html/plot_kmeans_orig_coord_equal-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Now this clustering may look correct as Dainis stands away from other customers. However, acknowledge that caviar is a delicacy. In comparison with bread, it is consumed in considerably smaller amounts and it is priced considerably higher. Therefore, a difference of 1 kilo of caviar in customer purchasing habits is more important than a difference of the same weight of bread. A calculation of distances using original, or non-standardized, variables does not differentiate between caviar and bread, essentially assuming these two products are identical. <strong>To summarize, a higher variation in bread weights makes this product be more important for Euclidean distance calculations and, as a result, for clustering results.</strong></p>
<p>So the results of clustering depend on the variation of variables in the data set. We can resolve this problem by standardizing the data prior to the clustering.</p>
</div>
<div id="standardized-variables" class="section level1">
<h1>Standardized variables</h1>
<p>Different standardization methods are available, including z-standardization (also called z-score standardization) and range standardization. Z-standardization rescales each variable <span class="math inline">\(X\)</span> by subtracting its mean <span class="math inline">\(\bar{x}\)</span> and dividing by its standard deviation <span class="math inline">\(s\)</span>:</p>
<p><span class="math display">\[Z=\frac{X-\bar{x}}{s}.\]</span></p>
<p>After z-standardization each variable has a mean <span class="math inline">\(\bar{z}\)</span> of 0 and a standard deviation <span class="math inline">\(s\)</span> of 1. Z-standardization can be done with <code>scale()</code>.</p>
<p>In cluster analysis, however, range standardization (e.g., to a range of 0 to 1) typically works better <span class="citation">(Milligan and Cooper 1988)</span>. Range standardization requires subtracting the minimum value and then dividing it by the range (i.e., the difference between the maximum and minimum value):</p>
<p><span class="math display">\[R = \frac{X - X_{min}}{X_{max} - X_{min}}\]</span></p>
<p>We can write a one-line function to perform range standardization:</p>
<pre class="r"><code>standardize_range &lt;- function(x) {(x - min(x)) / (max(x) - min(x))}</code></pre>
<p>Then apply our new function to each column:</p>
<pre class="r"><code>groceries_num_scaled &lt;- apply(groceries_num, 
                              MARGIN = 2, 
                              FUN = standardize_range)
groceries_num_scaled</code></pre>
<pre><code>##          caviar_kg  bread_kg
## Artis   0.00000000 1.0000000
## Baiba   0.09090909 0.8181818
## Cintija 0.90909091 0.5454545
## Dainis  1.00000000 0.0000000</code></pre>
<p>Notice that each column now has a range of one.</p>
<p>Repeat k-means clustering using <em>standardized</em> variables:</p>
<pre class="r"><code>set.seed(1)
kmeans_scaled &lt;- kmeans(groceries_num_scaled, centers = 2)</code></pre>
<p>The results have changed. Artis and Baiba are now in cluster 1 - customers buying 9-10 kilos of bread and almost no caviar. Cintija and Dainis are in cluster 2 - customers buying less bread but loving caviar.</p>
<p><img src="/post/2019-10-22-effect-of-not-standardizing-variables-in-k-means-clustering_files/figure-html/plot_kmeans_scaled_standardized_measurements-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<div id="results" class="section level1">
<h1>Results</h1>
<p>Below is the side-by-side illustration of the effect of variable standardization on k-means clustering. Clustering plotted on the left uses original variables and does not differentiate between a kilo of caviar from a kilo of bread. Clustering plotted on the right uses standardized variables and takes into account the fact that caviar is a delicacy purchased in considerably smaller volumes than bread.</p>
<p><img src="/post/2019-10-22-effect-of-not-standardizing-variables-in-k-means-clustering_files/figure-html/plot_kmeans_combined-1.png" width="768" style="display: block; margin: auto;" /></p>
</div>
<div id="conclusions" class="section level1">
<h1>Conclusions</h1>
<p>Perform exploratory data analysis prior to clustering and be aware of the differences in variation of variables. Variables with higher variation tend to have a higher impact on clustering. There is no single correct method of standardization. <span class="citation">(James et al. 2017, 400)</span> suggest trying several different choices and looking for the one with the most useful or interpretable solution. <span class="citation">(Milligan and Cooper 1988)</span> show that the range standardization typically works better for hierarchical clustering, and that z-standardization may even be significantly worse than no standardization in the presence of outliers. If there are outliers, a possible alternative is to use the median absolute deviation instead of the standard deviation <span class="citation">(Spector 2011, 160)</span>.</p>
<hr>
<p>I would appreciate any comments or suggestions. Please leave them below, no login required if you check “I’d rather post as a guest”.</p>
</div>
<div id="references" class="section level1 unnumbered">
<h1>References</h1>
<div id="refs" class="references">
<div id="ref-james_witten_hastie_tibshirani_2017">
<p>James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2017. <em>An Introduction to Statistical Learning: With Applications in R</em>. Springer. <a href="http://faculty.marshall.usc.edu/gareth-james/ISL/">http://faculty.marshall.usc.edu/gareth-james/ISL/</a>.</p>
</div>
<div id="ref-milligan_cooper">
<p>Milligan, Glenn W., and Martha C. Cooper. 1988. “A Study of Standardization of Variables in Cluster Analysis.” <em>SpringerLink</em>. Springer-Verlag. <a href="https://doi.org/10.1007/BF01897163">https://doi.org/10.1007/BF01897163</a>.</p>
</div>
<div id="ref-stat_133">
<p>Spector, Phil. 2011. “Stat 133 Class Notes - Spring, 2011.” <a href="https://www.stat.berkeley.edu/~s133/all2011.pdf">https://www.stat.berkeley.edu/~s133/all2011.pdf</a>.</p>
</div>
</div>
</div>
