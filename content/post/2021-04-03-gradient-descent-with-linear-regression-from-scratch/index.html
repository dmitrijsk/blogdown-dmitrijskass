---
title: Gradient descent with linear regression from scratch in Python
author: Dmitrijs Kass
date: '2021-04-03'
slug: gradient-descent-with-linear-regression-from-scratch
categories:
  - Python
  - gradient descent
  - linear regression
tags: []
---

<script src="{{< blogdown/postref >}}index_files/header-attrs/header-attrs.js"></script>


<hr />
<p>Hyper-parameters are parameters that are not directly learnt within estimators. In scikit-learn they are passed as arguments to the constructor of the estimator classes.</p>
<hr />
<p><img src="images/surface.png" style="width:100.0%" /></p>
<p>We want to minimize a convex, continuous and differentiable cost function <span class="math inline">\(J(\mathbf{w})\)</span>. In this blog post we discuss the most popular “hill-climbing” algorithm, <strong>gradient descent</strong>, using linear regression, and build it from scratch in Python. A few highlights:</p>
<ul>
<li>Code for linear regression and gradient descent is generalized to work with a model <span class="math inline">\(y=w_0+w_1x_1+\dots+w_px_p\)</span> for any <span class="math inline">\(p\)</span>.</li>
<li>Gradient descent is implemented using an object-oriented approach.</li>
<li>Impact of the learning rate on convergence (divergence) is illustrated.</li>
</ul>
<div id="theoretical-idea-of-the-gradient-descent" class="section level1">
<h1>Theoretical idea of the gradient descent</h1>
<p>Throughout this post I use bold font for vectors (e.g., <span class="math inline">\(\mathbf{w}\)</span>, <span class="math inline">\(\mathbf{x}\)</span>) and regular font for scalars and functions (e.g., <span class="math inline">\(w_0\)</span>, <span class="math inline">\(x_0\)</span>).</p>
<div id="taylor-expansion" class="section level2">
<h2>Taylor expansion</h2>
<p>Simplify the function you would like to minimize by using the first-order Taylor polynomial. Provided that the norm <span class="math inline">\(\lVert \mathbf{s} \rVert_2\)</span> is small (i.e., <span class="math inline">\(\mathbf{w}+\mathbf{s}\)</span> is very close to <span class="math inline">\(\mathbf{w}\)</span>), we can linearly approximate the function <span class="math inline">\(J(\mathbf{w} + \mathbf{s})\)</span> by its first derivative:</p>
<p><span class="math display">\[J(\mathbf{w}+\mathbf{s}) \approx J(\mathbf{w}) + \nabla J(\mathbf{w})^T \mathbf{s},\]</span></p>
<p>where <span class="math inline">\(\nabla J(\mathbf{w})\)</span> is the gradient of <span class="math inline">\(J\)</span>. This approximation is valid only when the step size <span class="math inline">\(\mathbf{s}\)</span> is small. We will return to this in the learning rate discussion.</p>
<div style="padding:10px;background-color:#EBEBEB;line-height:1.3;">
<p><strong>Note:</strong> We may also use the second-order Taylor polynomial to make a quadratic approximation of the function. Examples are Newton and Gauss-Newton methods. Second-order optimization methods consume more computation power and are less popular for machine learning model training.</p>
</div>
</div>
<div id="gradient" class="section level2">
<h2>Gradient</h2>
<p>The gradient vector</p>
<p><span class="math display">\[
\nabla J(\mathbf{w})
= \frac{\partial J (\mathbf{w})}{\partial \mathbf{w}}
= \left[\begin{array}{c}
\dfrac{\partial J (\mathbf{w})}{\partial w_0}\\
\dfrac{\partial J (\mathbf{w})}{\partial w_1}\\
\vdots \\
\dfrac{\partial J (\mathbf{w})}{\partial w_p}
\end{array}\right] 
\in \mathbb{R}^{p+1 \times 1}
\]</span></p>
<p>gives the direction of steepest ascent on the surface of the function <span class="math inline">\(J\)</span>, and the rate of change in this direction is <span class="math inline">\(\lVert \nabla J(\mathbf{w}) \rVert\)</span>. <a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a></p>
<!-- <p style="font-family:'Merriweather';color:blue;font-size:70%;"> -->
<!-- TODO: Add a surface plot with 3 arrows: in the direction of $w_0$, $w_1$ and the gradient vector. E.g., [here](https://physics.stackexchange.com/questions/368634/direction-of-velocity-vector-in-3d-space) -->
<!-- </p> -->
</div>
<div id="convergence" class="section level2">
<h2>Convergence</h2>
<p>Consequently, <span class="math inline">\(-\nabla J(\mathbf{w})\)</span> points in the direction of the steepest descent. Setting <span class="math inline">\(\mathbf{s} = -\alpha \nabla J(\mathbf{w})\)</span> for a <em>sufficiently small</em> <span class="math inline">\(\alpha&gt;0\)</span> guarantees to decrease the function<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a>:</p>
<p><span class="math inline">\(\underset{after\ one\ update}{\underbrace{J(\mathbf{w} + (-\alpha \nabla J(\mathbf{w}))}} \approx J(\mathbf{w}) - \underset{&gt;0}{\underbrace{\alpha\overset{&gt;0}{\overbrace{ \nabla J(\mathbf{w})^T \nabla J(\mathbf{w})}}}} &lt; \underset{before}{\underbrace{J(\mathbf{w})}}\)</span></p>
<p>So the iterations of steepest descent are:</p>
<p><span class="math display">\[\mathbf{w}^{(i+1)} \leftarrow \mathbf{w}^{(i)} - \alpha \nabla J(\mathbf{w}^{(i)}).\]</span></p>
</div>
<div id="algorithm" class="section level2">
<h2>Algorithm</h2>
<!-- https://www.overleaf.com/project/607b3c198b0e8e3c5fa3f21c -->
<center>
<img src="images/algo.png" style="width:100.0%" />
</center>
<p style="font-family:&#39;Merriweather&#39;;color:blue;font-size:70%;">
Note: <strong>do</strong> … <strong>while</strong> would be more suitable to avoid <span class="math inline">\(\mathbf{w}^{(-1)}\)</span>. (2021-04-18)
</p>
<p>We will implement this algorithm with linear regression using the squared loss. Let’s now define the linear regression and optimization problems.</p>
</div>
</div>
<div id="linear-regression" class="section level1">
<h1>Linear regression</h1>
<div id="model" class="section level2">
<h2>Model</h2>
<p>The linear regression model assumes that the numerical output variable <span class="math inline">\(y\)</span> can be described as an affine combination of the <span class="math inline">\(p\)</span> features <span class="math inline">\(x_1, \dots, x_p\)</span> plus a noise term <span class="math inline">\(\epsilon\)</span>,</p>
<p><span class="math display">\[y = w_0 + w_1x_1 + \dots + w_px_p + \epsilon.\]</span>
The coefficients <span class="math inline">\(\mathbf{w} = [w_0, w_1, \dots, w_p]^T\)</span> are called parameters, weights or coefficients of the model.</p>
<p>Prepend a feature vector <span class="math inline">\(\mathbf{x}\)</span> with a constant 1</p>
<p><span class="math display">\[\mathbf{x} = [1, x_1, \dots, x_p]^T\]</span></p>
<p>to express the linear regression model compactly as</p>
<p><span class="math display">\[y = \mathbf{w}^T \mathbf{x} + \epsilon.\]</span></p>
<p>The predicted output variable <span class="math inline">\(\widehat{y}\)</span> for some feature vector <span class="math inline">\(\mathbf{x}\)</span> using learned parameters <span class="math inline">\(\mathbf{\widehat{w}}\)</span> is obtained with</p>
<p><span class="math display">\[\widehat{y} = \mathbf{\widehat{w}}^T \mathbf{x}.\]</span>
The parameters are learned by minimizing the cost function <span class="math inline">\(J(\mathbf{w})\)</span>,</p>
<p><span class="math display">\[\widehat{\mathbf{w}} = \arg\min_\mathbf{w} J(\mathbf{w}).\]</span></p>
<p>We solve this optimization problem with the gradient descent.</p>
</div>
<div id="cost-function-and-its-gradient" class="section level2">
<h2>Cost function and its gradient</h2>
<p>The squared error loss</p>
<p><span class="math display">\[L(y_i,\widehat{y_i}) = (y_i - \widehat{y_i})^2\]</span></p>
<p>is a common loss function used for measuring the closeness of a prediction and the training data. The cost function is the average of the loss function evaluated on the training data:</p>
<p><span class="math display">\[J(\mathbf{w}) = \frac{1}{n} \sum_{i=1}^{n}(y_i - \widehat{y_i})^2,\]</span></p>
<p>where <span class="math inline">\(n\)</span> is the number of samples in the training data. It is a scalar-valued function. Quadratic nature of the loss function produces a <a href="https://en.wikipedia.org/wiki/Convex_function">convex</a> cost function.</p>
<p>If <span class="math inline">\(p=2\)</span> then</p>
<p><span class="math display">\[J(\mathbf{w}) = \frac{1}{n} \sum_{i=1}^{n}(y_i - (w_0 + w_1 x_{1i} + w_2 x_{2i}))^2,\]</span></p>
<p>and the gradient using the chain rule of calculus is</p>
<p><span class="math display">\[\nabla J(\mathbf{w})=
\left[\begin{array}{c}
\dfrac{\partial J (\mathbf{w})}{\partial w_0}\\
\dfrac{\partial J (\mathbf{w})}{\partial w_1}\\
\dfrac{\partial J (\mathbf{w})}{\partial w_2} 
\end{array}\right]=
\left[\begin{array}{c}
\frac{1}{n} 2 \sum_{i=1}^{n}(y_i - (w_0 + w_1 x_{1i} + w_2 x_{2i})) (-1)\\
\\
\frac{1}{n} 2 \sum_{i=1}^{n}(y_i - (w_0 + w_1 x_{1i} + w_2 x_{2i})) (-x_{1i})\\
\\
\frac{1}{n} 2 \sum_{i=1}^{n}(y_i - (w_0 + w_1 x_{1i} + w_2 x_{2i})) (-x_{2i})
\end{array}\right]\]</span></p>
<p style="font-family:&#39;Merriweather&#39;;color:blue;font-size:70%;">
Fix: 1/n missing both here and in the code. Same for AdaGrad. (2021-04-18).
</p>
<div style="padding:10px;background-color:#EBEBEB;line-height:1.3;">
<p><strong>Note:</strong> When I first encountered the gradient descent, I could not understand how the gradient can be calculated from discrete data points. <em>“Derivatives require continuity, but I am given just a few data points”</em>, I thought. But this is a wrong way to think about the gradient. The gradient is calculated using a given function (in our case, <span class="math inline">\(J(\mathbf{w})\)</span>) and has nothing to do with the actual data at this stage. Notice how we calculated the gradient above and we did not care about the data points yet. Data points are used <em>to evaluate</em> the gradient at <em>a specific</em> point (in our case, at <span class="math inline">\(\mathbf{w^{(i)}}\)</span> in each iteration <span class="math inline">\(i\)</span> of the gradient descent algorithm).</p>
</div>
</div>
</div>
<div id="implementation-in-python" class="section level1">
<h1>Implementation in Python</h1>
<p style="font-family:&#39;Merriweather&#39;;color:blue;font-size:70%;">
TODO: Plug-in algorithm that calculates the step. Then it will be easy to add Adagrad and other algorithms. (2021-04-22).
</p>
<p style="font-family:&#39;Merriweather&#39;;color:blue;font-size:70%;">
Fix: Comment for <span class="math inline">\(eps\)</span> is outdated. (2021-04-22)
</p>
<script src="https://gist.github.com/dmitrijsk/56cd0192432d76a2d1ca6a5d34361654.js"></script>
<div id="grad" class="section level3">
<h3><code>grad</code></h3>
<p>Returns the gradient vector. <code>d_intercept</code> is a partial derivative of the cost function w.r.t. the intercept <span class="math inline">\(w_0\)</span>. <code>d_x</code> is a partial derivative of the cost function w.r.t. to feature coefficients (<span class="math inline">\(w_1, \dots, w_p\)</span>).</p>
</div>
<div id="fit" class="section level3">
<h3><code>fit</code></h3>
<p>Line 55 initializes parameters <span class="math inline">\(\mathbf{w^{(0)}}\)</span> as a zero vector. Initialization can be done arbitrarily here because we are dealing with a convex loss function. For convex problems there is only one stationary point, which also is the global minimum.</p>
<div style="padding:10px;background-color:#EBEBEB;line-height:1.3;">
<p><strong>Note:</strong> Sometimes we can save computation time by <em>warm-starting</em> the optimization procedure with a good initial guess. For example, iterations <span class="math inline">\(2..k\)</span> of a <span class="math inline">\(k\)</span>-fold cross-validation may use parameters corresponding to a minimum loss in the previous iterations.</p>
</div>
<p>Lines 59-73 implement the gradient descent algorithm. Line 61 calculates the gradient, line 63 updates the parameters, line 66 calculates the value of the cost function. An early stoppage occurs when the Euclidean norm of the difference in parameters in the last two iterations becomes less than the tolerance level.</p>
</div>
</div>
<div id="learning-rate-and-convergence" class="section level1">
<h1>Learning rate and convergence</h1>
<p>“Setting the learning rate <span class="math inline">\(\alpha&gt;0\)</span> is a dark art. Only if it is sufficiently small will gradient descent converge”<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a>. Below we take a look at how a choice of <span class="math inline">\(\alpha\)</span> impacts convergence.</p>
<p>Let’s generate <span class="math inline">\(\mathbf{X}\)</span> and <span class="math inline">\(\mathbf{y}\)</span> data for the linear regression and use gradient descent to fit a straight line. Full code is available at my <a href="https://github.com/dmitrijsk/blogdown-dmitrijskass/tree/adagrad/content/post/2021-04-03-gradient-descent-with-linear-regression-from-scratch/code">GitHub repository</a>. Generated data looks as follows:</p>
<center>
<img src="images/1-generated-data.png" style="width:80.0%" />
</center>
<p>We fit the line using the gradient descent with <code>GradientDescentLinearRegression().fit(X, y, "standard")</code>. The last argument defines a variant of gradient descent to use and <code>standard</code> means the standard gradient descent. We will be adding more methods in the following blog posts.</p>
<div id="slow-convergence" class="section level2">
<h2>Slow convergence</h2>
<p>With learning rate <span class="math inline">\(\alpha=0.001\)</span> the algorithm slowly converges in 3058 iterations. The surface plot below is the surface of the cost function <span class="math inline">\(J(\mathbf{w})\)</span>. Red dots are updated parameters from initial <span class="math inline">\(\mathbf{w}^{(0)} = [0,0]\)</span> to optimal <span class="math inline">\(\mathbf{w}^{(3058)} = [0.24, 0.84]\)</span>.</p>
<center>
<img src="images/surface-method-standard-ls-0.001.png" style="width:100.0%" />
</center>
<p>Animation below shows how the line is slowly fitted though iterations 0 to 49.</p>
<center>
<img src="images/animation-method-standard-lr-0.001.gif" style="width:80.0%" />
</center>
</div>
<div id="good-convergence" class="section level2">
<h2>Good convergence</h2>
<p>Learning rate <span class="math inline">\(\alpha=0.015\)</span> seems to be a better guess and the algorithm converges in 292 iterations. After the first five steps the parameters are fairly close to the optimal solution.</p>
<center>
<img src="images/surface-method-standard-lr-0.015.png" style="width:100.0%" />
</center>
<p>The animation below illustrates the last statement - after the first five iterations the line barely moves.</p>
<center>
<img src="images/animation-method-standard-lr-0.015.gif" style="width:80.0%" />
</center>
</div>
<div id="jumps" class="section level2">
<h2>Jumps</h2>
<p>When the learning rate is large (but not too large yet), convergence happens in jumps. The change in <span class="math inline">\(\mathbf{w}\)</span> is large enough to take the updated parameters to the other side of the surface, surpassing the minimum point. With learning rate <span class="math inline">\(\alpha=0.05\)</span> the convergence shows this jumping behavior. Nevertheless, the algorithm converges because the magnitude of the gradient becomes smaller after each iteration. Convergence takes 97 iterations.</p>
<center>
<img src="images/surface-method-standard-lr-0.05.png" style="width:100.0%" />
</center>
<p>This surface plot also reveals that the gradient descent algorithm does not really slide down the surface. In fact, it is a step-wise movement:</p>
<ul>
<li>Start at <span class="math inline">\([ w_0^{(0)}, w_1^{(0)},J(w_0^{(0)}, w_1^{(0)}) ]\)</span>, which is <span class="math inline">\([0, 0, 13.5]\)</span>.</li>
<li>After the first update of the parameters, imagine the red dot moving in the <span class="math inline">\(XY\)</span> plane to <span class="math inline">\([ w_0^{(1)}, w_1^{(1)},J(w_0^{(0)}, w_1^{(0)}) ]\)</span>, which is <span class="math inline">\([-1.0, -27.3, 13.5]\)</span>.</li>
<li>Then the cost is recalculated using <span class="math inline">\([ w_0^{(1)}, w_1^{(1)}]\)</span> and the red dot descends to <span class="math inline">\([ w_0^{(1)}, w_1^{(1)},J(w_0^{(1)}, w_1^{(1)}) ]\)</span>, which is <span class="math inline">\([-1.0, -27.3, 6.3]\)</span>. But it is not guaranteed to descent. If the step size in <span class="math inline">\(XY\)</span> plane was too large, the red dot would actually ascend and the algorithm would diverge.</li>
</ul>
<p>Animation below illustrates the jumping behavior. With time, oscillations become smaller and eventually the algorithm converges.</p>
<center>
<img src="images/animation-method-standard-lr-0.05.gif" style="width:80.0%" />
</center>
</div>
<div id="divergence" class="section level2">
<h2>Divergence</h2>
<p>Learning rate <span class="math inline">\(\alpha=0.08\)</span> turns out to be too high. The magnitude of the gradient grows after the first iteration, which increases the step size for the next iteration. In this fashion, parameters <span class="math inline">\(\mathbf{w}^{(i)}\)</span> spiral away from the minimum very rapidly - compare the scale of <span class="math inline">\(z\)</span> axis on the previous plot and the one below with only seven iterations.</p>
<center>
<img src="images/surface-method-standard-lr-0.08.png" style="width:100.0%" />
</center>
<p>The fitted line does not seem to move during the first few iterations due to a huge scale of the plot. However, later we see that the line starts to swing wildly and quickly approaches a vertical where the cost becomes infinitely large.</p>
<center>
<img src="images/animation-method-standard-lr-0.08.gif" style="width:80.0%" />
</center>
<p><br></p>
<p>We would like to get rid of the dark art of guessing learning rates. A safe (but sometimes slow) choice is to set <span class="math inline">\(\alpha = \frac{i_0}{i}\)</span>, where <span class="math inline">\(i_0\)</span> is any initial positive value and <span class="math inline">\(i\)</span> is the iteration counter. This guarantees that it will eventually become small enough to converge<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a>. With the same generated data and <span class="math inline">\(i_0=1\)</span> the algorithm converged in 2384 iterations.</p>
<p>In the <a href="/2021/04/15/adagrad-adaptive-gradient-algorithm/">next post</a> we will look at the adaptive gradient algorithm AdaGrad, which performs more informative gradient-based learning by adapting the learning rate for <em>each feature individually</em> and does not require guessing <span class="math inline">\(\alpha\)</span>.</p>
<hr>
<p>I would appreciate any comments or suggestions. Please leave them below, no login required if you check “I’d rather post as a guest”.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Havens, A. Multivariate functions and partial derivatives. <a href="https://people.math.umass.edu/~havens/Partials.pdf">Link</a><a href="#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>“Gradient Descent (and Beyond)”. Lecture notes for the “Machine Learning for Intelligent Systems” course by Kilian Weinberger at Cornell University. <a href="https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote07.html">Link</a><a href="#fnref2" class="footnote-back">↩︎</a></p></li>
<li id="fn3"><p>Ibid.<a href="#fnref3" class="footnote-back">↩︎</a></p></li>
<li id="fn4"><p>Ibid.<a href="#fnref4" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
