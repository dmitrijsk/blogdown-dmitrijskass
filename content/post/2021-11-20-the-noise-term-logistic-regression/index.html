---
title: The noise term in logistic regression
author: ''
date: '2021-11-20'
slug: the-noise-term-logistic-regression
categories: []
tags: []
---

<script src="{{< blogdown/postref >}}index_files/header-attrs/header-attrs.js"></script>


<p>When using a linear regression, we asume that the data was generated by the <span class="math inline">\(y_i=\boldsymbol\theta^\top \mathbf{x}_i+\epsilon\)</span>, where <span class="math inline">\(\epsilon\)</span> is an explicit noise term. Usually it is assumed to be Normally distributed. This is a source of randomness in the linear regression model and that’s why data points don’t lie on the straight line.</p>
<p>What is the source of randomness in a binary logistic regression model? Recall that logistic regression is a classification model where a conditional probability for the positive class (labeled as <span class="math inline">\(1\)</span>) is</p>
<p><span class="math display">\[p(y = 1 | \mathbf{x}; \boldsymbol\theta) = \text{logistic}(\boldsymbol\theta^\top\mathbf{x}) = 1 / (1 + e^{-\boldsymbol\theta^\top\mathbf{x}}),\]</span></p>
<p>where <span class="math inline">\(y\)</span> is a class label, <span class="math inline">\(\boldsymbol\theta\)</span> is a vector of model parameters and <span class="math inline">\(\mathbf{x}\)</span> is a set of features. The dot product <span class="math inline">\(\boldsymbol\theta^\top\mathbf{x}\)</span> is usually referred to as a logit.</p>
<p>Notice that there is no explicit noise term here. The randomness in logistic regression setting originates from that fact that the class labels of data points follow Bernoulli distribution. Therefore, the probability of a data point being of a positive class is <span class="math inline">\(\text{Bernoulli(logistic}(\boldsymbol\theta^\top\mathbf{x})\text{)}\)</span>.</p>
<p>To see this in practice, let’s simulate a dataset for binary logistic regression using Python.</p>
<pre class="python"><code>import numpy as np
from sklearn.linear_model import LogisticRegression</code></pre>
<p>Define a logistic function to model a probability of a positive class.</p>
<pre class="python"><code>def logistic(z):
    return 1 / (1 + np.exp(-z))</code></pre>
<p>Define the true values for the parameter vector <span class="math inline">\(\boldsymbol\theta\)</span> of length <span class="math inline">\(p\)</span>.</p>
<pre class="python"><code># True theta coefficients.
theta = np.array([4, -2])
# Number of features.
p = len(theta)</code></pre>
<p>Generate <span class="math inline">\(n\)</span> data points from a uniform distribution on [0,1].</p>
<pre class="python"><code># Number of training data points.
n = 200
# Generate feature values from U[0,1].
np.random.seed(1)
X = np.random.rand(n, p)</code></pre>
<p>Calculate the probabilities of the positive class (labeled as <span class="math inline">\(1\)</span>).</p>
<pre class="python"><code># Calculate logits.
z = X @ theta.reshape(-1, 1)
# Calculate probabilities.
prob = logistic(z)</code></pre>
<p>Generate class labels from Bernoulli. Each data point uses its own probability from <code>prob</code>.</p>
<pre class="python"><code># Generate labels by sampling from Bernoulli(prob)
y = np.random.binomial(1, prob.flatten())</code></pre>
<p>Train a logistic regression model with no intercept and no regularization. By default, the penalty is <span class="math inline">\(L_2\)</span>.</p>
<pre class="python"><code># Train a logistic regression model.
model = LogisticRegression(fit_intercept = False, penalty = &quot;none&quot;).fit(X, y)</code></pre>
<p>With the random seed we set above, the learned <span class="math inline">\(\boldsymbol\theta\)</span> parameters are <span class="math inline">\([3.94\)</span> <span class="math inline">\(-1.93]\)</span>, which is close to the true vector <span class="math inline">\([4\)</span> <span class="math inline">\(-2]\)</span>.</p>
<p>Here is a full code:</p>
<script src="https://gist.github.com/dmitrijsk/69de066902d57bb4820bdc254313d1aa.js"></script>
<p>Additionally, we can make an illustration of the data points and the decision boundary of the logistic regression classifier:</p>
<center>
<img src="images/log-reg-decision-boundary.png" style="width:80.0%" />
</center>
<p>The decision boundary here was obtained by creating a grid of points and classifying them using the model obtained above. The full code for the plot is available <a href="">here</a>. Alternatively we could explicitly estimate the equation of the linear decision boundary. As an example, <a href="https://scipython.com/blog/plotting-the-decision-boundary-of-a-logistic-regression-model/">this page</a> contains a nice explanation of how to do it. Just note that in our model we assumed that the intercept is zero.</p>
<hr>
<p>I would appreciate any comments or suggestions. Please leave them below, no login required if you check “I’d rather post as a guest”.</p>
